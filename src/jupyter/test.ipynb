{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import scipy.optimize\n",
    "import scipy.sparse\n",
    "\n",
    "from networkx.utils import np_random_state\n",
    "\n",
    "from src.python._process_params import _process_params\n",
    "from src.python.getMatrix import getMatrixByName\n",
    "from src.python.vis.visGraph import visGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from networkx.drawing.layout.py\n",
    "@np_random_state(10)\n",
    "def spring_layout(\n",
    "    G,\n",
    "    k=None,\n",
    "    pos=None,\n",
    "    fixed=None,\n",
    "    iterations=50,\n",
    "    threshold=1e-4,\n",
    "    weight=\"weight\",\n",
    "    scale=1,\n",
    "    center=None,\n",
    "    dim=2,\n",
    "    seed=None,\n",
    "    method=\"FR\",  # ! added method = \"FR\" or \"RS\"\n",
    "):\n",
    "    G, center = _process_params(G, center, dim)\n",
    "\n",
    "    if fixed is not None:\n",
    "        if pos is None:\n",
    "            raise ValueError(\"nodes are fixed without positions given\")\n",
    "        for node in fixed:\n",
    "            if node not in pos:\n",
    "                raise ValueError(\"nodes are fixed without positions given\")\n",
    "        nfixed = {node: i for i, node in enumerate(G)}\n",
    "        fixed = np.asarray([nfixed[node] for node in fixed if node in nfixed])\n",
    "\n",
    "    if pos is not None:\n",
    "        # Determine size of existing domain to adjust initial positions\n",
    "        dom_size = max(coord for pos_tup in pos.values() for coord in pos_tup)\n",
    "        if dom_size == 0:\n",
    "            dom_size = 1\n",
    "        pos_arr = seed.rand(len(G), dim) * dom_size + center\n",
    "\n",
    "        for i, n in enumerate(G):\n",
    "            if n in pos:\n",
    "                pos_arr[i] = np.asarray(pos[n])\n",
    "    else:\n",
    "        pos_arr = None\n",
    "        dom_size = 1\n",
    "\n",
    "    if len(G) == 0:\n",
    "        return {}\n",
    "    if len(G) == 1:\n",
    "        return {nx.utils.arbitrary_element(G.nodes()): center}\n",
    "\n",
    "    # ! Changed a lot\n",
    "    A = nx.to_scipy_sparse_array(G, weight=weight, dtype=\"f\")\n",
    "    if k is None and fixed is not None:\n",
    "        # We must adjust k by domain size for layouts not near 1x1\n",
    "        nnodes, _ = A.shape\n",
    "        k = dom_size / np.sqrt(nnodes)\n",
    "    return _sparse_fruchterman_reingold(\n",
    "        A, k, pos_arr, fixed, iterations, threshold, dim, seed, method\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.utils import np_random_state\n",
    "from src.python.cost import cost\n",
    "import scipy as sp\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# Copied from networkx.drawing.layout.py\n",
    "@np_random_state(7)\n",
    "def _sparse_fruchterman_reingold(\n",
    "    A,\n",
    "    k=None,\n",
    "    pos=None,\n",
    "    fixed=None,\n",
    "    iterations=50,\n",
    "    threshold=1e-4,\n",
    "    dim=2,\n",
    "    seed=None,\n",
    "    method=\"FR\",\n",
    "    verbose=True,\n",
    "):\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "    try:\n",
    "        nnodes, _ = A.shape\n",
    "    except AttributeError as err:\n",
    "        msg = \"fruchterman_reingold() takes an adjacency matrix as input\"\n",
    "        raise nx.NetworkXError(msg) from err\n",
    "\n",
    "    if pos is None:\n",
    "        # random initial positions\n",
    "        pos = np.asarray(seed.rand(nnodes, dim), dtype=A.dtype)\n",
    "    else:\n",
    "        # make sure positions are of same type as matrix\n",
    "        pos = pos.astype(A.dtype)\n",
    "\n",
    "    # no fixed nodes\n",
    "    if fixed is None:\n",
    "        fixed = []\n",
    "\n",
    "    # optimal distance between nodes\n",
    "    if k is None:\n",
    "        k = np.sqrt(1.0 / nnodes)\n",
    "\n",
    "    if method == \"FR\":\n",
    "        # make sure we have a LIst of Lists representation\n",
    "        try:\n",
    "            A = A.tolil()\n",
    "        except AttributeError:\n",
    "            A = (sp.sparse.coo_array(A)).tolil()\n",
    "\n",
    "        # the initial \"temperature\" is about .1 of domain area (=1x1)\n",
    "        # this is the largest step allowed in the dynamics.\n",
    "        t = max(max(pos.T[0]) - min(pos.T[0]), max(pos.T[1]) - min(pos.T[1])) * 0.1\n",
    "        # simple cooling scheme.\n",
    "        # linearly step down by dt on each iteration so last iteration is size dt.\n",
    "        dt = t / (iterations + 1)\n",
    "\n",
    "        displacement = np.zeros((dim, nnodes))\n",
    "        for iteration in range(iterations):\n",
    "            displacement *= 0\n",
    "            # loop over rows\n",
    "            for i in range(A.shape[0]):\n",
    "                if i in fixed:\n",
    "                    continue\n",
    "                # difference between this row's node position and all others\n",
    "                delta = (pos[i] - pos).T\n",
    "                # distance between points\n",
    "                distance = np.sqrt((delta**2).sum(axis=0))\n",
    "                # enforce minimum distance of 0.01\n",
    "                distance = np.where(distance < 0.01, 0.01, distance)\n",
    "                # the adjacency matrix row\n",
    "                Ai = A.getrowview(i).toarray()  # TODO: revisit w/ sparse 1D container\n",
    "                # displacement \"force\"\n",
    "                displacement[:, i] += (\n",
    "                    delta * (k * k / distance**2 - Ai * distance / k)\n",
    "                ).sum(axis=1)\n",
    "            # update positions\n",
    "            length = np.sqrt((displacement**2).sum(axis=0))\n",
    "            length = np.where(length < 0.01, 0.1, length)\n",
    "            delta_pos = (displacement * t / length).T\n",
    "            pos += delta_pos\n",
    "            # cool temperature\n",
    "            t -= dt\n",
    "            if verbose:\n",
    "                print(f\"{cost(pos,A,k)=}\")\n",
    "            if (np.linalg.norm(delta_pos) / nnodes) < threshold:\n",
    "                break\n",
    "            yield pos\n",
    "    elif method == \"L-BFGS-B\" or method == \"BFGS\" or method == \"CG\":\n",
    "        # make sure we have a coo_matrix representation\n",
    "        try:\n",
    "            A = A.tolil()\n",
    "        except AttributeError:\n",
    "            A = (sp.sparse.coo_array(A)).tolil()\n",
    "\n",
    "        k_inv = 1 / k\n",
    "\n",
    "        def cost_fun(x):\n",
    "            pos = x.reshape((nnodes, dim))\n",
    "            grad = np.zeros((nnodes, dim))\n",
    "            for i in range(nnodes):\n",
    "                delta = pos[i] - pos\n",
    "                assert np.all(delta[i] == 0.0)\n",
    "                distance = np.linalg.norm(delta, axis=1)\n",
    "                distance = np.where(distance < 0.01, 0.01, distance)\n",
    "                distance_inv = 1 / distance\n",
    "                Ai = A.getrow(i).toarray().flatten()\n",
    "                coefficient1 = Ai * distance * k_inv - (k * distance_inv) ** 2\n",
    "                grad[i] = coefficient1 @ delta\n",
    "            ret = cost(pos, A, k)\n",
    "            print(f\"{ret=}\")\n",
    "            return ret, grad.ravel()\n",
    "\n",
    "        pos_hist = []\n",
    "        res = sp.optimize.minimize(\n",
    "            cost_fun,\n",
    "            pos.ravel(),\n",
    "            method=method,\n",
    "            jac=True,\n",
    "            options={\"maxiter\": iterations, \"disp\": verbose},\n",
    "            callback=lambda x: pos_hist.append(x.reshape((nnodes, dim))),\n",
    "        )\n",
    "\n",
    "        for pos in pos_hist:\n",
    "            yield pos\n",
    "\n",
    "        if verbose:\n",
    "            print(\"         WarnFlag and message: %d\" % res.status, res.message)\n",
    "            print(\"         Current function value: %f\" % res.fun)\n",
    "            print(\"         Iterations: %d\" % res.nit)\n",
    "\n",
    "    elif method == \"myCG\":\n",
    "        from scipy.optimize._optimize import _line_search_wolfe12\n",
    "\n",
    "        # make sure we have a coo_matrix representation\n",
    "        try:\n",
    "            A = A.tolil()\n",
    "        except AttributeError:\n",
    "            A = (sp.sparse.coo_array(A)).tolil()\n",
    "\n",
    "        k_inv = 1 / k\n",
    "\n",
    "        def f(x: np.ndarray) -> float:\n",
    "            pos = x.reshape((nnodes, dim))\n",
    "            ret = cost(pos, A, k)\n",
    "            print(f\"{ret=}\")\n",
    "            return ret\n",
    "\n",
    "        def myfprime(x: np.ndarray) -> np.ndarray:\n",
    "            pos = x.reshape((nnodes, dim))\n",
    "            grad = np.zeros((nnodes, dim))\n",
    "            for i in range(nnodes):\n",
    "                delta = pos[i] - pos\n",
    "                assert np.all(delta[i] == 0.0)\n",
    "                distance = np.linalg.norm(delta, axis=1)\n",
    "                distance = np.where(distance < 0.01, 0.01, distance)\n",
    "                distance_inv = 1 / distance\n",
    "                Ai = A.getrow(i).toarray().flatten()\n",
    "                coefficient1 = Ai * distance * k_inv - (k * distance_inv) ** 2\n",
    "                grad[i] = coefficient1 @ delta\n",
    "            return grad.ravel()\n",
    "\n",
    "        x0 = pos.ravel()\n",
    "        gtol = 1e-5\n",
    "        disp = True\n",
    "        c1 = 1e-4\n",
    "        c2 = 0.4\n",
    "\n",
    "        old_fval = f(x0)\n",
    "        gfk = myfprime(x0)\n",
    "\n",
    "        k_cg = 0\n",
    "        xk = x0\n",
    "        old_old_fval = old_fval + np.linalg.norm(gfk) / 2\n",
    "\n",
    "        warnflag = 0\n",
    "        pk = -gfk\n",
    "        gnorm = np.amax(np.abs(gfk))\n",
    "\n",
    "        sigma_3 = 0.01\n",
    "\n",
    "        while (gnorm > gtol) and (k_cg < iterations):\n",
    "            deltak = np.dot(gfk, gfk)\n",
    "\n",
    "            cached_step = [None]\n",
    "\n",
    "            def polak_ribiere_powell_step(alpha, gfkp1=None):\n",
    "                xkp1 = xk + alpha * pk\n",
    "                if gfkp1 is None:\n",
    "                    gfkp1 = myfprime(xkp1)\n",
    "                yk = gfkp1 - gfk\n",
    "                beta_k = max(0, np.dot(yk, gfkp1) / deltak)\n",
    "                pkp1 = -gfkp1 + beta_k * pk\n",
    "                gnorm = np.amax(np.abs(gfkp1))\n",
    "                return (alpha, xkp1, pkp1, gfkp1, gnorm)\n",
    "\n",
    "            def descent_condition(alpha, xkp1, fp1, gfkp1):\n",
    "                # Polak-Ribiere+ needs an explicit check of a sufficient\n",
    "                # descent condition, which is not guaranteed by strong Wolfe.\n",
    "                #\n",
    "                # See Gilbert & Nocedal, \"Global convergence properties of\n",
    "                # conjugate gradient methods for optimization\",\n",
    "                # SIAM J. Optimization 2, 21 (1992).\n",
    "                cached_step[:] = polak_ribiere_powell_step(alpha, gfkp1)\n",
    "                alpha, _, pk, gfk, gnorm = cached_step\n",
    "\n",
    "                # Accept step if it leads to convergence.\n",
    "                if gnorm <= gtol:\n",
    "                    return True\n",
    "\n",
    "                # Accept step if sufficient descent condition applies.\n",
    "                return np.dot(pk, gfk) <= -sigma_3 * np.dot(gfk, gfk)\n",
    "\n",
    "            try:\n",
    "                alpha_k, _, _, old_fval, old_old_fval, gfkp1 = _line_search_wolfe12(\n",
    "                    f,\n",
    "                    myfprime,\n",
    "                    xk,\n",
    "                    pk,\n",
    "                    gfk,\n",
    "                    old_fval,\n",
    "                    old_old_fval,\n",
    "                    c1=c1,\n",
    "                    c2=c2,\n",
    "                    amin=1e-100,\n",
    "                    amax=1e100,\n",
    "                    extra_condition=descent_condition,\n",
    "                )\n",
    "                print(f\"{alpha_k=}\")\n",
    "            except RuntimeError:\n",
    "                # Line search failed to find a better solution.\n",
    "                warnflag = 2\n",
    "                break\n",
    "\n",
    "            # Reuse already computed results if possible\n",
    "            if alpha_k == cached_step[0]:\n",
    "                alpha_k, xk, pk, gfk, gnorm = cached_step\n",
    "            else:\n",
    "                alpha_k, xk, pk, gfk, gnorm = polak_ribiere_powell_step(alpha_k, gfkp1)\n",
    "\n",
    "            k_cg += 1\n",
    "\n",
    "            yield xk.reshape((nnodes, dim))\n",
    "\n",
    "        fval = old_fval\n",
    "        if disp:\n",
    "            if warnflag == 2:\n",
    "                msg = \"pr_loss\"\n",
    "            elif k_cg >= iterations:\n",
    "                warnflag = 1\n",
    "                msg = \"maxiter\"\n",
    "            elif np.isnan(gnorm) or np.isnan(fval) or np.isnan(xk).any():\n",
    "                warnflag = 3\n",
    "                msg = \"nan\"\n",
    "            else:\n",
    "                msg = \"success\"\n",
    "            print(\"         WarnFlag and message: %d\" % warnflag, msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "\n",
    "    elif method == \"CGVR\":\n",
    "        from scipy.optimize._optimize import _line_search_wolfe12\n",
    "\n",
    "        # make sure we have a coo_matrix representation\n",
    "        try:\n",
    "            A = A.tolil()\n",
    "        except AttributeError:\n",
    "            A = (sp.sparse.coo_array(A)).tolil()\n",
    "\n",
    "        disp = True\n",
    "        gtol = 1e-5\n",
    "        c1 = 1e-4\n",
    "        c2 = 0.4\n",
    "        c3 = 0.01\n",
    "        xk = pos\n",
    "\n",
    "        cache_f = [None]\n",
    "\n",
    "        def f_i(xk_i: np.ndarray, i: int) -> Tuple[float, np.ndarray]:\n",
    "            assert xk_i.shape == (dim,)\n",
    "            if np.all(xk_i == cache_f[0]):\n",
    "                return cache_f[1:]\n",
    "            delta = xk_i - xk\n",
    "            delta[i].fill(0.0)\n",
    "            distance = np.linalg.norm(delta, axis=1)\n",
    "            distance = np.where(distance < 0.01, 0.01, distance)\n",
    "            Ai = A.getrow(i).toarray().flatten()\n",
    "            EPS = 1e-10\n",
    "            val = np.sum(Ai * (distance**3) / (3 * k) - (k**2) * np.log(distance + EPS))\n",
    "            grad = (Ai * distance / k - (k / distance) ** 2) @ delta\n",
    "            cache_f[:] = (xk_i, val, grad)\n",
    "            return cache_f[1:]\n",
    "\n",
    "        gfk = [np.zeros(dim) for _ in range(nnodes)]\n",
    "        old_fval = [0.0 for _ in range(nnodes)]\n",
    "        old_old_fval = [0.0 for _ in range(nnodes)]\n",
    "        for i in range(nnodes):\n",
    "            old_fval[i], gfk[i] = f_i(xk[i])\n",
    "            old_old_fval[i] = old_fval[i] + np.linalg.norm(gfk[i]) / 2\n",
    "\n",
    "        warnflag = 0\n",
    "\n",
    "        k_cg = 0\n",
    "        while k_cg < iterations:\n",
    "            uk = [np.zeros(dim) for _ in range(nnodes)]\n",
    "            for i in range(nnodes):\n",
    "                uk[i] = f_i(xk[i])[1]\n",
    "            xk_0 = np.copy(xk)\n",
    "            p = [-gfk_i for gfk_i in gfk]\n",
    "            gnorm = max(np.amax(np.abs(gfk_i)) for gfk_i in gfk)\n",
    "            if gnorm <= gtol:\n",
    "                break\n",
    "\n",
    "            deltak = sum(np.dot(gfk_i, gfk_i) for gfk_i in gfk)\n",
    "\n",
    "            for i in range(nnodes):\n",
    "                cached_step = [None]\n",
    "\n",
    "                def polak_ribiere_powell_step(alpha, gfkp1_i=None):\n",
    "                    xkp1_i = xk[i] + alpha * pk[i]\n",
    "                    if gfkp1_i is None:\n",
    "                        gfkp1_i = f_i(xkp1_i)[1]\n",
    "                    yk = gfkp1_i - gfk[i]\n",
    "                    beta_k = max(0, np.dot(yk, gfkp1_i) / deltak)\n",
    "                    pkp1 = -gfkp1_i + beta_k * pk\n",
    "                    gnorm = np.amax(np.abs(gfkp1_i))\n",
    "                    return (alpha, xkp1, pkp1, gfkp1_i, gnorm)\n",
    "\n",
    "                def descent_condition(alpha, _xkp1, _fp1, gfkp1):\n",
    "                    # Polak-Ribiere+ needs an explicit check of a sufficient\n",
    "                    # descent condition, which is not guaranteed by strong Wolfe.\n",
    "                    #\n",
    "                    # See Gilbert & Nocedal, \"Global convergence properties of\n",
    "                    # conjugate gradient methods for optimization\",\n",
    "                    # SIAM J. Optimization 2, 21 (1992).\n",
    "                    cached_step[:] = polak_ribiere_powell_step(alpha, gfkp1)\n",
    "                    alpha, _, pk, gfk, gnorm = cached_step\n",
    "\n",
    "                    # Accept step if it leads to convergence.\n",
    "                    if gnorm <= gtol:\n",
    "                        return True\n",
    "\n",
    "                    # Accept step if sufficient descent condition applies.\n",
    "                    return np.dot(pk, gfk) <= -c3 * np.dot(gfk, gfk)\n",
    "\n",
    "                try:\n",
    "                    alpha_k, _, _, old_fval[i], old_old_fval[i], gfkp1 = (\n",
    "                        _line_search_wolfe12(\n",
    "                            lambda xk_i: f_i(xk_i, i)[0],\n",
    "                            lambda xk_i: f_i(xk_i, i)[1],\n",
    "                            xk[i],\n",
    "                            pk[i],\n",
    "                            gfk[i],\n",
    "                            old_fval[i],\n",
    "                            old_old_fval[i],\n",
    "                            c1=c1,\n",
    "                            c2=c2,\n",
    "                            amin=1e-100,\n",
    "                            amax=1e100,\n",
    "                            extra_condition=descent_condition,\n",
    "                        )\n",
    "                    )\n",
    "                    print(f\"{alpha_k=}\")\n",
    "                except RuntimeError:\n",
    "                    # Line search failed to find a better solution.\n",
    "                    warnflag = 2\n",
    "                    break\n",
    "\n",
    "                # Reuse already computed results if possible\n",
    "                if alpha_k == cached_step[0]:\n",
    "                    alpha_k, xk, pk, gfk, gnorm = cached_step\n",
    "                else:\n",
    "                    alpha_k, xk, pk, gfk, gnorm = polak_ribiere_powell_step(\n",
    "                        alpha_k, gfkp1\n",
    "                    )\n",
    "\n",
    "            k_cg += 1\n",
    "\n",
    "            yield xk.reshape((nnodes, dim))\n",
    "\n",
    "        fval = old_fval\n",
    "        if disp:\n",
    "            if warnflag == 2:\n",
    "                msg = \"pr_loss\"\n",
    "            elif k_cg >= iterations:\n",
    "                warnflag = 1\n",
    "                msg = \"maxiter\"\n",
    "            elif np.isnan(gnorm) or np.isnan(fval) or np.isnan(xk).any():\n",
    "                warnflag = 3\n",
    "                msg = \"nan\"\n",
    "            else:\n",
    "                msg = \"success\"\n",
    "            print(\"         WarnFlag and message: %d\" % warnflag, msg)\n",
    "            print(\"         Current function value: %f\" % fval)\n",
    "            print(\"         Iterations: %d\" % k)\n",
    "\n",
    "    else:\n",
    "        raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret=np.float64(14284.042775097787)\n",
      "ret=np.float64(11100.294607333897)\n",
      "ret=np.float64(3813.8346665761064)\n",
      "alpha_k=np.float64(0.0029043966788128827)\n",
      "ret=np.float64(390221.1810293812)\n",
      "ret=np.float64(3059.8415855778567)\n",
      "ret=np.float64(31275.174184878557)\n",
      "ret=np.float64(2083.4173716333544)\n",
      "alpha_k=np.float64(0.005788957819632335)\n",
      "ret=np.float64(9340.421186670099)\n",
      "ret=np.float64(1818.4737750036486)\n",
      "alpha_k=np.float64(0.010606575794178803)\n",
      "ret=np.float64(1754.8779343736119)\n",
      "ret=np.float64(1559.8307267640448)\n",
      "alpha_k=np.float64(0.05516197837260731)\n",
      "ret=np.float64(2129.5763473535376)\n",
      "ret=np.float64(1414.3100378916083)\n",
      "ret=np.float64(1460.5048392960923)\n",
      "ret=np.float64(1376.2595692643135)\n",
      "alpha_k=np.float64(0.038502182616530406)\n",
      "ret=np.float64(1754.0724968769764)\n",
      "ret=np.float64(1255.8258464643966)\n",
      "ret=np.float64(1249.2859036411355)\n",
      "ret=np.float64(1288.8284333207805)\n",
      "ret=np.float64(1227.3160717880842)\n",
      "alpha_k=np.float64(0.025728142461905176)\n",
      "ret=np.float64(1244.9887249014794)\n",
      "ret=np.float64(1081.0156949749953)\n",
      "alpha_k=np.float64(0.03425499639767483)\n",
      "ret=np.float64(1227.4040161566484)\n",
      "ret=np.float64(960.9176226070324)\n",
      "alpha_k=np.float64(0.028502644703722435)\n",
      "ret=np.float64(1045.2582338844163)\n",
      "ret=np.float64(858.5881613369743)\n",
      "alpha_k=np.float64(0.03548952876722616)\n",
      "ret=np.float64(843.0579913070962)\n",
      "ret=np.float64(757.4620060252722)\n",
      "alpha_k=np.float64(0.05359419605597141)\n",
      "         WarnFlag and message: 1 maxiter\n",
      "         Current function value: 757.462006\n",
      "         Iterations: 0\n",
      "L-BFGS-B: 1.2999807950109243e-06[sec]\n",
      "      CG: 9.00006853044033e-07[sec]\n",
      "    myCG: 12.281873600004474[sec]\n"
     ]
    }
   ],
   "source": [
    "mat = getMatrixByName(\"jagmesh1\")\n",
    "\n",
    "if scipy.sparse.issparse(mat):\n",
    "    mat.setdiag(0)\n",
    "    mat.eliminate_zeros()\n",
    "    mat.data = np.abs(mat.data)\n",
    "else:\n",
    "    mat[np.diag_indices_from(mat)] = 0\n",
    "    mat.data = np.abs(mat.data)\n",
    "\n",
    "G = nx.Graph(mat)\n",
    "\n",
    "\n",
    "if False:\n",
    "    for iteration, pos in enumerate(spring_layout(G, method=\"FR\", iterations=50)):\n",
    "        if iteration % 5 == 0:\n",
    "            visGraph(G, pos)\n",
    "    # visGraph(G, list(spring_layout(G, method=\"FR\", iterations=250))[-1])\n",
    "if True:\n",
    "    import time\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    # list(spring_layout(G, seed=0, method=\"L-BFGS-B\", iterations=10))\n",
    "    t1 = time.perf_counter()\n",
    "    # list(spring_layout(G, seed=0, method=\"CG\", iterations=10))\n",
    "    t2 = time.perf_counter()\n",
    "    list(spring_layout(G, seed=0, method=\"myCG\", iterations=10))\n",
    "    t3 = time.perf_counter()\n",
    "\n",
    "    print(f\"L-BFGS-B: {t1-t0}[sec]\")\n",
    "    print(f\"      CG: {t2-t1}[sec]\")\n",
    "    print(f\"    myCG: {t3-t2}[sec]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
