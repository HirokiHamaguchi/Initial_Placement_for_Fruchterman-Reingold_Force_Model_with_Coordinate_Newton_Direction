\documentclass[dvipdfmx,journal]{IEEEtran}

\usepackage[ruled, vlined]{algorithm2e}    % \begin{algorithm2e}
\usepackage{amsmath}        % \begin{align*}
\usepackage{amssymb}        % \mathbb{A}
\usepackage{amsthm}         % \newtheorem
\usepackage{bm,bbm}         % \bm{A}, \bbm{1}
\usepackage{booktabs}       % \toprule, \midrule, \bottomrule
\usepackage{subcaption}        % \captionsetup
\usepackage{enumitem}       % \begin{enumerate}[label=(\alph*)]
\usepackage{hyperref}       % \href{URL}{text}
\usepackage{ifthen}         % \ifthenelse
\usepackage{lipsum}         % \lipsum
\usepackage{mathrsfs}       % \mathscr{A}
\usepackage{mathtools}      % \mathrlap
\usepackage{optidef}        % \begin{mini*}{x}{f(x)}{}{}
\usepackage{orcidlink}      % \orcidlink
\usepackage{physics}        % \qty, \norm, \abs
\usepackage{subfiles}       % \subfile{file}
\usepackage{thm-restate}    % \begin{restatable}{theorem}{thm}
\usepackage{tikz}           % \begin{tikzpicture}
\usepackage{xparse}         % \NewDocumentCommand
% \usepackage{calc}         % \setlength
% \usepackage{cancel}       % \cancel
% \usepackage{parskip}      % \setlength{\parskip}{0.5em}
% \usepackage{csvsimple}    % \csvautotabular
% \usepackage{diagbox}      % \diagbox
% \usepackage{dsfont}       % \mathds{1}
% \usepackage{epsfig}       % \epsfig
% \usepackage{fancybx}      % \ovalbox
% \usepackage{float}        % \begin{figure}[H]
% \usepackage{lipsum}       % \lipsum
% \usepackage{listings}     % \begin{lstlisting}
\usepackage{makecell}     % \makecell{L1\L2}
% \usepackage{multicol}     % \begin{multicols}{2}
% \usepackage{multirow}     % \multirow
% \usepackage{nicematrix}   % \begin{NiceMatrix}
% \usepackage{qcircuit}     % \Qcircuit
% \usepackage{siunitx}      % \SI{1}{\second}
% \usepackage{stfloats}     % \begin{figure*}
% \usepackage{subcaption}   % \begin{subfigure}
% \usepackage{ulem}         % \sout
% \usepackage{wrapfig}      % \begin{wrapfigure}
% \usepackage[all]{xy}      % \xymatrix
% \usepackage[dvipdfmx]{graphicx}
% \usepackage[square, sort, comma, numbers]{natbib}

\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}

\definecolor{cA}{HTML}{0072BD}
\definecolor{cB}{HTML}{EDB120}
\definecolor{cC}{HTML}{77AC30}
\definecolor{cD}{HTML}{D95319}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\brown}[1]{\textcolor{brown}{#1}}
\newcommand{\black}[1]{\textcolor{black}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\Img}[1]{\mathrm{Im}\qty(#1)}
\newcommand{\Ker}[1]{\mathrm{Ker}\qty(#1)}
\newcommand{\Supp}[1]{\mathrm{supp}\qty(#1)}
\newcommand{\Rank}[1]{\mathrm{rank}\qty(#1)}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
% C++ (https://tex.stackexchange.com/questions/4302/prettiest-way-to-typeset-c-cplusplus)
\newcommand{\Cpp}{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}{\textbf{++}}}}
% https://tex.stackexchange.com/questions/28836/typesetting-the-define-equals-symbol
\newcommand{\defeq}{\coloneqq}
\newcommand{\eqdef}{\eqqcolon}
% https://tex.stackexchange.com/questions/5502/how-to-get-a-mid-binary-relation-that-grows
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}

% https://tex.stackexchange.com/questions/564216/newcommand-for-each-letter
\ExplSyntaxOn
\NewDocumentCommand{\definealphabet}{mmmm}{
\int_step_inline:nnn{`#3}{`#4}{
\cs_new_protected:cpx{#1 \char_generate:nn{##1}{11}}{
\exp_not:N #2{\char_generate:nn{##1}{11}}}}}
\ExplSyntaxOff

\definealphabet{bb}{\mathbb}{A}{Z}
\definealphabet{rm}{\mathrm}{A}{Z}
\definealphabet{cal}{\mathcal}{A}{Z}
\definealphabet{frak}{\mathfrak}{a}{z}
% \definealphabet{scr}{\mathscr}{A}{Z}
% \definealphabet{frak}{\mathfrak}{A}{Z}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}

% https://qiita.com/rityo_masu/items/efd44bc8f9229e014237
\allowdisplaybreaks[4]

\usetikzlibrary{
  3d,
  % fit,
  calc,
  math,
  matrix,
  patterns,
  backgrounds,
  arrows.meta,
  shapes.geometric,
  decorations.pathmorphing,
}

% \providecommand{\main}{.}
\newboolean{isMain}
\setboolean{isMain}{true}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

\title{
    An Initial Placement
    for Fruchterman--Reingold Layout
    by \red{Subspace Newton Direction}
}
\author{
    Hiroki Hamaguchi\,\orcidlink{0009-0005-7348-1356}
    Naoki Marumo\,\orcidlink{0000-0002-7372-4275}
    Akiko Takeda
    % <-this % stops a space
}
\maketitle

\begin{abstract}
    Graph drawing is a fundamental task for visualizing graph structures, with the Fruchterman--Reingold (FR) layout being one of the most widely used layouts.
    This layout can be interpreted as a solution to a continuous optimization problem.
    Typically, this problem is solved using the FR algorithm, a gradient descent-like method, or the L-BFGS, quasi-Newton method.
    However, these methods are computationally expensive per iteration, which makes achieving high-quality visualizations for large-scale graphs challenging.

    In this paper, to accelerate the optimization process, we propose a new initial placement obtained by subspace newton direction.
    We first reformulate the problem as a discrete optimization problem using a hexagonal lattice
    and then iteratively move a randomly selected vertex along the Newton direction defined in the subspace.
    We can use the FR algorithm or L-BFGS method to obtain the final placement.

    We demonstrate the effectiveness of our proposed approach through experiments, highlighting the potential of subspace-based methods for graph drawing tasks.
    Additionally, we suggest combining our method with other graph drawing techniques for further improvement.
    We hope that this work will inspire future research of subspace-based methods not only in graph drawing but also in broader graph-related applications.
\end{abstract}

\begin{IEEEkeywords}
    Graph Drawing,
    Optimization,
    Fruchterman--Reingold Layout,
    L-BFGS method,
    subspace method
\end{IEEEkeywords}

\section{Introduction}\label{sec:introduction}

\IEEEPARstart{G}{raph} is a mathematical structure representing pairwise relationships between objects, and graph drawing is one of the fundamental tasks in graph theory.
Indeed, numerous kinds of layouts and algorithms have been proposed for graph drawing~\cite{tutteHowDrawGraph1963,chrobakLineartimeAlgorithmDrawing1995,sugiyamaMethodsVisualUnderstanding1981,ghassemitoosiSimulatedAnnealingPreProcessing2016}.
Among these, one of the most popular strategies is force-directed algorithms.

In force-directed algorithms, we model a graph as a system of particles with forces acting between them.
This class of algorithms includes the Kamada--Kawai (KK) layout~\cite{kamadaAlgorithmDrawingGeneral1989},
Eades' spring embedder~\cite{eades1984heuristic}, and the Fruchterman--Reingold (FR) layout~\cite{fruchtermanGraphDrawingForcedirected1991,kobourovSpringEmbeddersForce2012}, which is the central focus of this study.

The FR layout, also known as spring layout or spring-electrical model, is one of the most widely used layouts.
It is also implemented in many modern graph drawing libraries such as NetworkX~\cite{hagberg2008exploring}, Graphviz~\cite{ellsonGraphvizOpenSource2002}, and igraph~\cite{csardiIgraphSoftwarePackage2006}.

However, both the KK layout and the FR layout suffer from high computational complexity, specifically $\order{n^2}$ per iteration as it is, where $n$ denotes the number of vertices.
This computational burden makes it challenging to achieve high-quality visualizations for large-scale graphs.

To address this kind of burden, we can leverage several methods, such as approximating the $n$-body simulation using multipole expansions~\cite{greengardFastAlgorithmParticle1987} or the Barnes--Hut approximation~\cite{barnesHierarchicalLogForcecalculation1986}, gradually refining the layouts using a multilevel approach~\cite{Hu2006EfficientHF} and employing stress majorization~\cite{gansnerGraphDrawingStress2005}.

Another approach is to directly accelerate the optimization process, which aligns with the aim of our work.
Recent research has accelerated the algorithms through various methods, including adaptation to GPU parallel architectures~\cite{gajdosParallelFruchtermanReingold2016}, utilizing numerical optimization techniques such as L-BFGS~\cite{6183577}, and Stochastic Gradient Descent (SGD)~\cite{8419285}.

\begin{figure}[t]
    \centering
    \begin{tabular}{cc}
        \large{\textbf{\textsf{FR}}}                                       & \large{\textbf{\textsf{L-BFGS}}}   \\
        \begin{minipage}{0.45\columnwidth}
            \centering
            \includegraphics[width=0.85\columnwidth]{jagmesh1/jagmesh1_FR.png}
        \end{minipage} &
        \begin{minipage}{0.45\columnwidth}
            \centering
            \includegraphics[width=0.85\columnwidth]{jagmesh1/jagmesh1_L_BFGS.png}
        \end{minipage}                                   \\
        \large{\textbf{\textsf{proposed}}}                                 & \large{\textbf{\textsf{+ L-BFGS}}} \\
        \begin{minipage}{0.45\columnwidth}
            \centering
            \includegraphics[width=0.85\columnwidth]{jagmesh1/jagmesh1_SN.png}
        \end{minipage}
                                                                           &
        \begin{minipage}{0.45\columnwidth}
            \centering
            \includegraphics[width=0.85\columnwidth]{jagmesh1/jagmesh1_SN_L_BFGS.png}
        \end{minipage}
    \end{tabular}
    \begin{tikzpicture}[overlay, remember picture]
        \draw[-{Stealth[length=2mm,width=3mm]}, line width=1mm] (-0.3,1.55) -- (+0.3,1.55);
    \end{tikzpicture}
    \caption{
        Comparison of the FR algorithm, L-BFGS method, and the proposed initial placement for the \texttt{jagmesh1} dataset.}
    \label{table:four_images}
\end{figure}

\red{
    Based on such advances, in this paper, we propose a new initial placement for the FR layout.
    Our goal is to accelerate the optimization process by leveraging the inherent structure of the problem.
    To achieve this, we optimize the position of $n$ vertices one by one with the Newton direction, which is defined in the subspace $\bbR^2$ of the entire variable space $\bbR^{2 \times n}$.
    In this way, we provide a high-quality initial placement in advance, improving the overall optimization process.
    We also demonstrate its effectiveness through experiments.
}

The rest is as follows.
In Sec.~\ref{sec:preliminary}, we define the optimization problem for the FR layout.
In Sec.~\ref{sec:RQ}, we present our research question based on previous works.
In Sec.~\ref{sec:algorithm}, we propose a new initial placement algorithm.
In Sec.~\ref{sec:experiment}, we show the experimental results.
\red{In Sec.~\ref{sec:challenges}, we discuss the potential of the subspace methods, the key concept of our research.}
Finally, we discuss future work and conclude the paper in Sec.~\ref{sec:discussion}.

\section{Preliminary}\label{sec:preliminary}

\begin{figure}[t]
    \centering
    \includegraphics[height=5.5cm]{fr_layout/fr_layout.pdf}
    \caption{
        (Top) Fruchterman--Reingold layout. It models $\order{n^2}$ forces between all pairs of vertices.
        (Bottom) Forces $F_{i,j}^\mathrm{a}(d)$ and $F^\mathrm{r}(d)$ work between $v_i$ and $v_j$. The equilibrium of them is achieved at $d = k/\sqrt[3]{w_{i,j}}$, which equals $k$ when $w_{i,j} = 1$.
    }
    \label{fig:frLayout}
\end{figure}

In this section, we formulate the FR layout as a continuous optimization problem and introduce the conventional approaches to this problem, namely the FR algorithm and the L-BFGS method.

\subsection{Fruchterman--Reingold layout}\label{ssec:frLayout}

Let $\bbR_{> 0} \defeq \qty{ x \in \bbR \mid x > 0 }, \quad \bbR_{\geq 0} \defeq \qty{ x \in \bbR \mid x \geq 0 }$, and let $W = (w_{i,j}) \in \bbR_{\geq 0}^{n \times n}$ be an adjacency matrix of a graph $G_W = (V, E)$, where $V = \qty{v_i \mid 1 \leq i \leq n}$ is a set of vertices and $E = \qty{(v_i, v_j) \mid w_{i,j} > 0}$ is a set of edges. We call $w_{i,j}$ as a weight of the edge $(v_i, v_j)$.

We will only consider undirected connected graphs with non-negative weights.
Although the FR algorithm in NetworkX, for example, can handle directed unconnected graphs with negative weights, this paper does not focus on such cases.
For directed graphs, slight modifications of algorithms or converting graphs to undirected ones may be effective.
For unconnected graphs, algorithms can be applied to each connected component independently.
When negative weights are present, the optimization problem~\eqref{eq:fr} defined below can be unbounded, but with non-negative weights and the connectivity of $G$, the problem is always bounded and solvable.
In summary, the conditions for $W$ is formulated as follows:
\begin{equation}\label{eq:WCondition}
    W \in \bbR_{\geq 0}^{n \times n}, \quad W = W^\top \quad \text{and $G_W$ is connected}.
\end{equation}

Fruchterman and Reingold~\cite{fruchtermanGraphDrawingForcedirected1991} proposed a force-directed layout called the Fruchterman--Reingold (FR) layout, as known as a spring layout~\cite{hagberg2008exploring} or spring-electrical model~\cite{Hu2006EfficientHF}.
Let $x_i \in \bbR^2$ be the position of the vertex $v_i \in V$, and $X = (x_1, \dots, x_n) \in \bbR^{2 \times n}$ be the placement of the graph.
Let $\norm{\cdot}$ denote the Euclidean distance in $\bbR^2$. For a parameter $k$ and a distance $d_{i,j} \defeq \norm{x_i - x_j}$ between two vertices $v_i$ and $v_j$, the attraction force $F_{i,j}^\mathrm{a}\colon \bbR_{> 0} \to \bbR$ and the repulsion force $F^\mathrm{r}\colon \bbR_{> 0} \to \bbR$ is defined as
\begin{equation*}
    F_{i,j}^\mathrm{a}(d) \defeq \frac{w_{i,j} d^2}{k}, \quad F^\mathrm{r}(d) \defeq -\frac{k^2}{d}.
\end{equation*}
FR layout seeks the equilibrium of the forces between all pairs of vertices, as shown in Fig.~\ref{fig:frLayout}.

We can also interrupt forces by its scalar potential~\cite{6183577}, in other words, energy $E_{i,j}: \bbR_{> 0} \to \bbR$, which is defined by
\begin{align}
    E_{i,j}^\mathrm{a}(d) & \defeq \int_{0}^{d} F_{i,j}^\mathrm{a}(r) \dd{r} = \frac{w_{i,j} d^3}{3k},\notag \\
    E^\mathrm{r}(d)       & \defeq \int_{\infty}^{d} F^\mathrm{r}(r) \dd{r} = -k^2\log{d}, \notag            \\
    E_{i,j}(d)            & \defeq E_{i,j}^\mathrm{a}(d) + E^\mathrm{r}(d). \label{eq:Eijr}
\end{align}
As a remark, this energy function $E_{i,j}$ is convex and minimized when $d = k/\sqrt[3]{w_{i,j}}$, but $E_{i,j}$ is not Lipschitz continuous.
Plus, a function $x_i \mapsto E_{i,j}(\norm{x_i - x_j})$ is not convex for a fixed $x_j$. Refer to Fig.~\ref{fig:energy3d}.

\begin{figure}[t]
    \centering
    \includegraphics[height=5.5cm]{energy_3d/energy_3d.png}
    \caption{Energy function $E_{i,j}(\norm{x_i - x_j})$ for $x_j=(0,0)$, $w_{i,j} = 1$ and $k = 1$. Although $E_{i,j}$ is convex as a function of $d$, it is not convex as a function of $x_i$. As $x_i$ approaches $x_j$, the energy function diverges.}
    \label{fig:energy3d}
\end{figure}

Now, the optimization problem for FR layout can be formulated as the minimization of the energy function $f: \bbR^{2 \times n} \to \bbR$, as known as the stress of the graph:
\begin{mini}
    {X \in \bbR^{2 \times n}}
    {f(X) \defeq \sum_{i<j} E_{i,j}(\norm{x_i - x_j}).}
    {\label{eq:fr}}
    {}
\end{mini}
Seeking an equilibrium of the forces is equivalent to seeking a local minimum of the energy function $E_{i,j}$ for all pairs of vertices.
In the following, we will discuss this optimization problem~\eqref{eq:fr}.

\subsection{Fruchterman--Reingold algorithm}\label{ssec:frAlgorithm}

The Fruchterman--Reingold algorithm~\cite{fruchtermanGraphDrawingForcedirected1991}, which is the original force-directed algorithm for this layout, can be regarded as a most standard approach for solving the optimization problem~\eqref{eq:fr}.
As mentioned in Ref.~\cite{tunkelang1999numerical}, the FR algorithm can be regarded as a variant of gradient descent (steepest descent) method for the energy function $f$ with a cooling global temperature $t$.

Let denote $f_i: \bbR^2 \to \bbR$ as the energy function for the vertex $v_i$ at $x_i$:
\begin{equation*}
    \red{f_i(x_i) \defeq \sum_{j \neq i} E_{i,j}(\norm{x_i - x_j}).}
\end{equation*}
The gradient of $f_i$ is
\begin{equation}\label{eq:gradientFi}
    \nabla f_i(x_i) = \sum_{j \neq i} \qty(\frac{w_{i,j}\norm{x_i - x_j}}{k} - \frac{k^2}{\norm{x_i - x_j}^2}) (x_i-x_j),
\end{equation}
which is the sum of forces acting on the vertex $v_i$.

We show the pseudo-code of the FR algorithm in Algorithm~\ref{alg:fr}. It is based on the original pseudo-code~\cite{fruchtermanGraphDrawingForcedirected1991} and implementation in NetworkX~\cite{hagberg2008exploring} with some omitted details.

\begin{algorithm}[ht]
    \caption{Fruchterman--Reingold algorithm}
    \label{alg:fr}
    \KwIn{Graph $G_W = (V, E)$}
    \KwOut{Point placement $X = (x_1, \dots, x_n)$}

    $\text{Define parameters } \red{k, t, N_\mathrm{iter}^\mathsf{FR}, \Delta_t \defeq t/N_\mathrm{iter}^\mathsf{FR}}$\;
    Define initial placement of $v_i \in V$ randomly\;
    \For{\red{$n_\mathrm{iter} \gets 1$ \KwTo $N_\mathrm{iter}^\mathsf{FR}$}}{
        $\text{compute gradient } \nabla f_i(x_i)$ for all $v_i \in V$\;
        $x_i \gets x_i - t \frac{\nabla f_i(x_i)}{
                \norm{\nabla f_i(x_i)}}$ for all $v_i \in V$\;
        $t \gets t - \Delta_t$\;
        \If{\text{convergence condition is satisfied}}{
            \textbf{break}\;
        }
    }
    \Return $X$\;
\end{algorithm}

In Algorithm~\ref{alg:fr}, the initial placement of points is determined randomly. Under proper input normalization, each point is uniformly distributed within a unit square in general.

The parameter $t$ denotes the temperature, which governs the step size along the steepest descent. As the temperature gradually decreases, the algorithm converges to a particular placement, though this placement is not necessarily the optimal solution.

\subsection{L-BFGS method}\label{ssec:lbfgs}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.713\columnwidth]{comparison/comparison_FRandLBFGS.pdf}
    \caption{
        Comparison of the FR algorithm and the L-BFGS method.
        Although the FR algorithm moves vertices in a descent direction with a fixed step size (blue arrows), the L-BFGS method adjusts them differently since it utilizes approximated inverse Hessian of $f$ (red arrows).
    }
    \label{fig:comparisonFRandLBFGS}
\end{figure}

Another approach to solve the optimization problem~\eqref{eq:fr} is to use the Limited-memory Broyden--Fletcher--Goldfarb--Shanno (L-BFGS) method~\cite{6183577}.
The L-BFGS method is in a family of quasi-Newton methods, and iterates procedures at most $N_\mathrm{iter}^{\mathsf{L-BFGS}}$ times.
Using only a few recent gradient vector, L-BFGS method approximates the inverse Hessian of the objective function, which is necessary to determine a descent direction~\cite{liuLimitedMemoryBFGS1989}.
L-BFGS is known to be very efficient for large-scale optimization problems, and the superior performance of the L-BFGS method to the FR algorithm reported in Ref.\cite{6183577} also indicates this fact. Refer to Fig.~\ref{fig:comparisonFRandLBFGS} for a comparison.

There are many available implementations of L-BFGS method, such as SciPy~\cite{2020SciPy-NMeth} and C++ L-BFGS~\cite{qiuYixuanLBFGSpp2024,okazakiChokkanLiblbfgs2024}, which we used in our experiments.

For the optimization problem~\eqref{eq:fr}, we can apply the L-BFGS method via flattening the matrix $X \in \bbR^{2 \times n}$ to a vector $\overline{X} \in \bbR^{2n}$.
However, it is worth noting that this method ignores the structure of $X$ and treats it just as a general optimization problem.
Thus, we can expect room for improvement by leveraging what we have ignored in this L-BFGS method.

\section{Research Question}\label{sec:RQ}

Now, we declare the research question of this paper: ``Can we accelerate the optimization process for the FR layout by leveraging the inherent structure of the problem?''.

In the previous section, we presented the FR algorithm and L-BFGS method. However, both methods face specific challenges during optimization, leading to inefficiencies and slow convergence, as detailed in Sec.~\ref{ssec:twist}.
Addressing these challenges forms a part of our research question, and we aim to develop a new algorithm capable of overcoming these limitations. To achieve this goal, we will review key prior studies in Sections~\ref{ssec:sgd} and~\ref{ssec:introNewton}.

\subsection{``twist'' in the optimization process}\label{ssec:twist}

\begin{figure}[t]
    \centering
    \begin{tabular}{ccc}
        \textsf{sfdp}                                                   & \textsf{fdp} & \textsf{spring\_layout} \\
        \includegraphics[width=0.273\columnwidth]{cycle/cycle_sfdp.png} &
        \includegraphics[width=0.273\columnwidth]{cycle/cycle_fdp.png}  &
        \includegraphics[width=0.273\columnwidth]{cycle/cycle_fr.png}                                            \\
    \end{tabular}
    \caption{
        Comparison of \textsf{sfdp} in Graphviz, \textsf{fdp} in Graphviz, and \textsf{spring\_layout} in NetworkX for the cycle graph with $\abs{V} = 300$, \texttt{cycle300}.
        We run every algorithm with the default parameters, but both \textsf{fdp} and \textsf{spring\_layout} fail to visualize the cycle beautifully.
    }
    \label{fig:cycle}
\end{figure}

First, we observe the challenges that appeared in existing methods.
One of the most challenging aspects of optimizing the FR layout is addressing the issue of ``twist'' in general.

The term ``twist'' does not refer to a mathematically rigorous concept; rather, it describes situations where unnecessary intersections of edges or tangled structures appear in visual representations.
Although the term ``twist'' is uncommon, works such as Ref.~\cite{cheongSnapshotVisualizationComplex2018} also mentioned this ``twist'' phenomenon. Fig.~\ref{fig:cycle} illustrates cycle graph examples of this situation.

The presence or absence of ``twists'' can impact optimization efficiency.
For example, when we draw a cycle graph, even if the current point placement is far from optimal, the optimization method proceeds relatively smoothly if there are no ``twists'' in the graph.
However, when ``twist'' exists like in Fig~\ref{fig:cycle}, mutual interactions may diminish the gradient, causing the optimization to stagnate.
Therefore, we can predict that providing an initial placement of the graph that resolves ``twist'' as much as possible in advance can significantly influence the efficiency of the optimization process.

\subsection{SGD for KK layout}\label{ssec:sgd}

Moreover, the effectiveness of Stochastic Gradient Descent (SGD) for Kama--Kawai (KK) layout is well-documented in~\cite{8419285}. This work has some implications for the FR layout.

The KK layout is an energy-based layout, which minimizes the energy function $f^{\mathrm{KK}}$ defined as
\begin{equation*}
    f^{\mathrm{KK}}_{i,j}(d) \defeq w_{i,j} (d-l_{i,j})^2, \;
    f^{\mathrm{KK}}(X) \defeq \sum_{i<j} f^{\mathrm{KK}}_{i,j}(\norm{x_i - x_j}),
\end{equation*}
where $l_{i,j}$ is the optimal distance between $v_i$ and $v_j$ calculated from the shortest path distance in the graph $G_W$.

The SGD method is an optimization method often used in machine learning to minimize a sum of differentiable functions, and Ref.~\cite{8419285} applied its modified version to the KK layout.
It randomly selects pairs of vertices $(i,j)$ and adjusts their positions to minimize $f^{\mathrm{KK}}_{i,j}$ along the gradient direction with a learning rate $\eta$.

However, in contrast to the KK layout, the FR layout assigns the function $E_{i,j}(d)=-k^2\log{d}$ to all $(i,j) \notin E (\iff w_{i,j}=0)$, making SGD less effective for the FR layout.
Nevertheless, the superiority of SGD in the KK layout, which focuses on ``randomly selected edge,'' suggests that an optimization method focusing on ``randomly selected vertex'' may also be effective in the FR layout, especially when we reduce the ``twist'' in advance.
This observation motivates us to propose a new approach combined with idea in the next subsection.

\subsection{Newton Direction and Subspace Newton Direction}\label{ssec:introNewton}

\red{
    In this subsection, we will explain Newton direction and subspace Newton direction, which are the key concepts of our research.
    Let $f: \bbR^n \to \bbR$ be a convex function.
    The second order approximation of $f$ at $x \in \bbR^n$ is
    \begin{equation*}
        f(y) \approx f(x) + \nabla f(x)^\top (y - x) + \frac{1}{2} (y - x)^\top \nabla^2 f(x) (y - x).
    \end{equation*}
    Since $f$ is convex, the Hessian matrix $\nabla^2 f(x)$ is positive semi-definite.
    The argmin of this approximation $x^*$ satisfies
    \begin{align*}
             & \nabla f(x) + \nabla^2 f(x) (x^* - x) = 0 \\
        \iff & x^* = x - \nabla^2 f(x)^{-1} \nabla f(x).
    \end{align*}
    We call the direction $d = -\nabla^2 f(x)^{-1} \nabla f(x)$ as the Newton direction.
    Although Newton direction is the optimal direction for the approximation, it requires the computation of the inverse Hessian $\nabla^2 f(x)^{-1} \in \bbR^{n \times n}$, posing a high computational cost for large-scale problems. Actually, the reason why L-BFGS method approximates the inverse Hessian is to avoid this computational cost.
}

\red{
    However, by combining this idea with the concept of subspace, the computational cost issue can be addressed in other ways.
    The computational cost of computing the inverse Hessian $\nabla^2 f(x)^{-1}$ increase with respect to the dimension $n$. Thus, by limiting the variable space to its subspaces, we can reduce the computational cost as well.
    We call the Newton direction defined in the subspace as the subspace Newton direction.
    This idea is well known as the subspace method or Randomized Subspace Newton (RSN)~\cite{NEURIPS2019_bc6dc48b}.
    Refer to Fig.~\ref{fig:subspace} for an illustration of the subspace method.
}

\red{
    Although directly applying this idea to the FR layout is challenging as we will discuss in Sec.~\ref{sec:challenges}, we leverage this subspace Newton direction to propose a new algorithm for the FR layout in the next section.
}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{subspace/subspace.pdf}
    \caption{\orange{TODO}}
    \label{fig:subspace}
\end{figure}

\section{Proposed Algorithm}\label{sec:algorithm}

To answer the research question above, we propose a new algorithm for the FR layout that utilizes the subspace Newton direction.
We describe our proposed method in three stages.
Firstly, in Section \ref{ssec:reduction}, we reformulate the optimization problem \eqref{eq:fr} into a simplified discrete optimization problem using a hexagonal lattice.
Secondly, in Section \ref{ssec:newtonDirection}, we present a method to solve the discrete optimization problem through continuous relaxation, using the Newton direction for vertices selected randomly, which is the core of our proposed algorithm.
Finally, in Section \ref{ssec:pseudoCode}, we show the complete framework of the proposed method, where we use the solution obtained from the previous step as the initial solution.

\subsection{reduction to the discrete optimization problem}\label{ssec:reduction}

First, we transform the optimization problem \eqref{eq:fr} into a constrained continuous optimization problem.
As written in Sec.~\ref{ssec:sgd}, the energy function $E_{i,j}$ is $-k^2\log{d}$ for all $(i,j) \notin E$.
Considering the sparsity of many practical graphs $(\abs{E} \ll \abs{V}^2)$, simplifying as follow is a reasonable approach:
\begin{mini}
    {X \in \bbR^{2 \times n}}
    {\sum_{(i,j)\in E} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k}- \sum_{i<j} k^2\log{\norm{x_i - x_j}}.}
    {\label{eq:frApprox}}
    {}
\end{mini}
Further, by converting the second term into a constraint, the problem \eqref{eq:frApprox} can be approximated such that the objective function can be computed with a complexity dependent on $\abs{E}$ rather than $\abs{V}^2$:
\begin{mini}
    {X \in \bbR^{2 \times n}}
    {f^{\mathrm{a}}(X) \defeq \sum_{(i,j)\in E} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k}}
    {\label{eq:frApprox2}}
    {}
    \addConstraint{d_{i,j}=\norm{x_i - x_j}}{\geq \epsilon,\quad}{\forall (i,j)\,(i<j)}
\end{mini}
where $\epsilon$ is a suitably chosen positive constant. This conversion is reasonable because $E^\mathrm{r}(d)=-k^2\log{d}$ is a convex function such that it decreases monotonically concerning $d$. Thus, for sufficiently large $d$, the value of $-k^2\log{d}$ does not grow excessively, ensuring the validity of the approximation.

However, problem \eqref{eq:frApprox2} still involves $\order{\abs{V}^2}$ constraints, which negates the advantage of computing the objective function with $\order{\abs{E}}$ complexity.
To further simplify, we incorporate the concept of fixed initial placements for the FR layout~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016}.
This study reports that a cycle initial placement obtained by Simulated Annealing (SA) brings a rapid convergence to a better solution in the FR layout.
Similarly, by simplifying problem \eqref{eq:frApprox2} with a fixed initial placement $Q$ whose points are separated by at least $\epsilon$, we obtain the following discrete optimization problem:
\begin{mini}
    {\pi: V \to Q}
    {\red{\sum_{(i,j)\in E} \frac{w_{i,j}\norm{\pi(v_i) - \pi(v_j)}^3}{3k}}}
    {\label{eq:frApprox3}}
    {}
    \addConstraint{\pi(v_i)}{\neq \pi(v_j),\quad}{\forall (i,j)\,(i<j)}
\end{mini}
It means that with a discrete set of points $Q$ such that the points are separated by at least $\epsilon$, we seek the best injection $\pi :V \to Q$ that minimizes the objective function $f^{\mathrm{a}}$.
By fixing the possible point placement in advance, we can skip the check of the $\order{\abs{V}^2}$ constraints, reducing the computational complexity to $\order{\abs{E}}$ and thus offering significant speedup.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{pi/pi.pdf}
    \caption{
        concept of $\pi$.
        The injection $\pi$ maps vertices $V$ to a discrete point placement $Q$.
        Apparently, among $\pi_1, \pi_2, \pi_3$, $\pi_3$ on the right one is the best mapping for the Problem~\eqref{eq:frApprox3}.
    }
    \label{fig:pi}
\end{figure}

As an example of such a discrete point placement $Q$, one could consider $n$ circles of radius $\epsilon$ packed in $\bbR^2$.
In this study, however, we adopt a hexagonal lattice.
The hexagonal lattice is known for its densest packing structure in space and offers computational simplicity.
See Figure \ref{fig:pi} for reference.
Although not directly related to this study, prior research~\cite{s22145179} has also pointed out the connection between FR layouts and hexagonal lattices.

\subsection{the Newton direction for discrete optimization}\label{ssec:newtonDirection}

Next, using the Newton direction of a randomly selected vertex, we solve the discrete optimization problem through continuous relaxation. The computation of the Newton direction in this part is essentially identical to the RSN method described in Sec.~\ref{ssec:introRSN}.

Let the objective function $f^{\mathrm{a}}_i(x_i)$ corresponding to a vertex $v_i$ be
\begin{equation*}
    f^{\mathrm{a}}_i(x_i) \defeq \sum_{j \neq i} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k}.
\end{equation*}
Using the Newton direction, we can take a candidate position of a vertex $v_i$ as
\begin{equation*}
    x_i^\mathrm{new} \gets \mathrm{round}\qty(x_i - \nabla^2 f^{\mathrm{a}}_i(x_i)^{-1} \nabla f^{\mathrm{a}}_i(x_i)),
\end{equation*}
where $\mathrm{round}(\hat{x})$ denotes the operation assigning $\hat{x}$ to the nearest point in the hexagonal lattice $Q$.

\red{
    Simply changing the assignment $\pi(v_i)$ from vertex $v_i$ to $Q$, updating from $x_i$ to $x_i^\mathrm{new}$, may not preserve the injectivity of $\pi$. To address this, consider the path from $x_i$ to $x_i^\mathrm{new}$, which can be determined using linear interpolation for line drawing~\cite{patelHexagonalGrids2013}. If each point on this path within $Q$ is assigned to vertices such as $v_i, v_j, \text{NaN}, v_k$, then by a cyclic shift, we reassign in the sequence $v_j, \text{NaN}, v_k, v_i$. This approach ensures that vertex $v_i$ can be reassigned to its new position $x_i^\mathrm{new}$ while maintaining the injectivity of $\pi$.
}
We illustrated the overall process in Fig.~\ref{fig:hex}.

This approach can obtain a high-quality initial solution for the optimization problem~\eqref{eq:fr}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{hex/hex.pdf}
    \caption{Visual explanation of the one iteration of the proposed algorithm. Step1. Compute the Newton direction for a randomly selected vertex. Step2. decide the path from the original position to the new position. Step3. move the vertex along the path.}
    \label{fig:hex}
\end{figure}

\subsection{pseudo code}\label{ssec:pseudoCode}

We presented the overall framework of the proposed method in Algorithm~\ref{alg:proposed}.
The output of this algorithm is not a solution to Problem~\eqref{eq:fr}, but rather a solution to the discrete optimization problem~\eqref{eq:frApprox3}.
To obtain the solution to Problem~\eqref{eq:fr}, the output must be further refined using the FR algorithm or the L-BFGS method.

\begin{algorithm}[ht]
    \caption{Proposed algorithm as initial placement for the FR layout}
    \label{alg:proposed}
    \KwIn{Graph $G_W = (V, E)$, subspace dimension $s$}
    \KwOut{Point placement $X = (x_1, \dots, x_n)$}

    define parameters $k, N_\mathrm{iter}^\mathsf{SN}$ and hexagonal lattice $Q$\;
    set $\pi$ as a injection from $V$ to $Q$ randomly\;
    \For{$j \gets 0$ \KwTo $N_\mathrm{iter}^\mathsf{SN}$}{
    $v_i \gets \text{randomly selected vertex from } V$\;
    $x_i \gets \pi(v_i)$\;
    $x_i^\mathrm{new} \gets \mathrm{round}(x_i - \nabla^2 f_i(x_i)^{-1} \nabla f_i(x_i))$\;
        $\textit{path} \gets \text{path from } x_i \text{ to } x_i^\mathrm{new}$\;
        update $\pi$ by cyclic shifting along the path\;
        \If{\text{convergence condition}}{
            \textbf{break}\;
        }
        }
    $x_i \gets \pi(v_i)$ for all $v_i \in V$\;
        \Return $X$
\end{algorithm}

\section{Numerical Experiment} \label{sec:experiment}

In this section, we evaluate the proposed algorithm by various numerical experiments based on the setup described in Sec.~\ref{ssec:setup}.
We conducted two types of experiments, based on Ref.~\cite{8419285}:
exhaustive experiments to evaluate the performance of the proposed algorithm in various situations in Sec.~\ref{ssec:exprAll},
and detailed experiments to investigate the behavior of the proposed algorithm in detail in Sec.~\ref{ssec:exprDetail}.

\subsection{Experimental Setup}\label{ssec:setup}

All numerical experiments in this section were conducted using \Cpp17 compiled by GCC 10.5.0 on a laptop computer powered by Intel(R) Core(TM) i7-10510U CPU with 16 GB RAM.

To implement FR algorithm and L-BFGS method, we referenced NetworkX version 3.3~\cite{hagberg2008exploring} and C++ L-BFGS~\cite{qiuYixuanLBFGSpp2024,okazakiChokkanLiblbfgs2024}, respectively. In particular, we used almost the same parameters as NetworkX's \textsf{spring\_layout} for the FR algorithm.
We also referenced the open-source code of the hexagonal grid from~\cite{patelHexagonalGrids2013}.
As a side note, we also used Graphviz version 2.43.0~\cite{ellsonGraphvizOpenSource2002} to draw Fig.~\ref{fig:cycle}.

We used the 4 algorithms: FR algorithm (\textsf{FR}), the proposed initialization plus FR algorithm (\textsf{SN-FR}), L-BFGS method (\textsf{L-BFGS}), and the proposed initialization plus L-BFGS method (\textsf{SN-L-BFGS}). \textsf{SN} represents the Subspace Newton, and we refer both \textsf{SN-FR} and \textsf{SN-L-BFGS} as \textsf{SN} in the following.

As parameters, we used $N_\mathrm{iter}^\mathsf{SN} = 3 \frac{\abs{V}^3}{\abs{E}}$ for the FR algorithm. Since the amortized time complexity of
Alg.~\ref{alg:proposed} is $\order{\frac{\abs{E}}{\abs{V}}}$, we can roughly expect that the computational time of the proposed algorithm is equivalent to 3 iterations of the FR algorithm.

All the codes are available at our GitHub~\cite{ThisPaperGitHub}.

\subsection{Exhaustive Experiment}\label{ssec:exprAll}

To begin with, we conducted an exhaustive experiment to evaluate the performance of the proposed algorithm with various graphs.

As a dataset, we used matrices from Sparse Matrix Collection~\cite{davis2011university} satisfying the condition~\eqref{eq:WCondition} with $\abs{V} \leq 1000$, in total 124 graphs.

The result is shown in Fig.~\ref{fig:individual}.
Almost all of the cases, the proposed algorithm performed better than random initialization.

There are a few cases where the proposed algorithm performed worse than random initialization, and we visualized such cases in Fig.~\ref{fig:can_187}, Fig.~\ref{fig:can_73}, Fig.~\ref{fig:iris_dataset_30NN}, and Fig.~\ref{fig:dwt_592}.
We can observe that the reasons why it was worse though these figures. \orange{todo}

\begin{figure*}[t]
    \centering
    \begin{minipage}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{overall/plot/diff_FR_15.pdf}
    \end{minipage}
    \begin{minipage}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{overall/plot/diff_L-BFGS_15.pdf}
    \end{minipage}
    \begin{minipage}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{overall/plot/diff_FR_50.pdf}
    \end{minipage}
    \begin{minipage}{\columnwidth}
        \centering
        \includegraphics[width=\columnwidth]{overall/plot/diff_L-BFGS_50.pdf}
    \end{minipage}
    \caption{
        Comparison of the proposed algorithm with the FR algorithm.
        The left plot shows the difference in the objective function value $f(X)$ between the proposed algorithm and the FR algorithm at the 15th iteration.
        The right plot shows the difference in the objective function value $f(X)$ between the proposed algorithm and the FR algorithm at the 50th iteration.
    }
    \label{fig:diff_L_BFGS}
\end{figure*}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/can_187_SN_FR_first.png}
    \end{minipage}
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/can_187_SN_FR_last.png}
    \end{minipage}
    \caption{\texttt{can\_187}}
    \label{fig:can_187}
\end{figure}
\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/can_73_SN_L_BFGS_first.png}
    \end{minipage}
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/can_73_SN_L_BFGS_last.png}
    \end{minipage}
    \caption{\texttt{can\_73}}
    \label{fig:can_73}
\end{figure}
\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/iris_dataset_30NN_SN_L_BFGS_first.png}
    \end{minipage}
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/iris_dataset_30NN_SN_L_BFGS_last.png}
    \end{minipage}
    \caption{\texttt{iris\_dataset\_30NN}}
    \label{fig:iris_dataset_30NN}
\end{figure}
\begin{figure}[t]
    \centering
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/dwt_592_SN_L_BFGS_first.png}
    \end{minipage}
    \begin{minipage}{0.49\columnwidth}
        \centering
        \includegraphics[width=0.5\columnwidth]{overall/viz/dwt_592_SN_L_BFGS_last.png}
    \end{minipage}
    \caption{\texttt{dwt\_592}}
    \label{fig:dwt_592}
\end{figure}


\subsection{detailed Experiment}\label{ssec:exprDetail}

We also conducted a detailed experiment to investigate the behavior of the proposed algorithm in detail. The result is shown in Fig.~\ref{fig:individual}.

The experiment details are as follows.
As parameters, we used $N_\mathrm{iter}^\mathsf{FR} = N_\mathrm{iter}^\mathsf{L-BFGS} = 200$ as the maximum number of iterations.
We tested with 7 graphs: \texttt{cycle300}, \texttt{jagmesh1}, \texttt{dwt\_1005}, \texttt{btree9}, \texttt{1138\_bus}, \texttt{dwt\_2680}, and \texttt{3elt}. \texttt{cycle300} is a cycle graph with 300 vertices, and btree9 is a perfect binary tree with $2^{9+1}-1=1023$ vertices. Other graphs are from Sparse Matrix Collection~\cite{davis2011university}, and these choices are based on the experiments conducted in Ref.~\cite{8419285}. Thus, although all the graphs are quite sparse so that $\abs{E} / (\abs{V}(V-1)/2)$ is less than 1\%, this is not an arbitrary choice. The important graphs often have such a sparsity.

We first explain what Fig.~\ref{fig:individual} represents.
The plots on the left illustrate the objective function values $f(X)$ on the vertical axis versus execution time on the horizontal axis, using 10 trials for each algorithm.
Faint crosses represent the results of non-\textsf{SN} algorithms, while faint circles represent the results of \textsf{SN} algorithms, plotted every 10 iterations for each trial.
The solid and dashed lines represent the average of these values across the 10 trials for each algorithm.
If one of the trials terminated before reaching $N_\mathrm{iter}^\mathsf{FR}$ or $N_\mathrm{iter}^\mathsf{L-BFGS}$ iterations, the line was plotted up to the minimum trial count achieved across all trials.
Each trial was conducted with a different seed, meaning that even deterministic algorithms, such as
\textsf{FR} or \textsf{L-BFGS}, converged to different local optima due to variations in the initial random placement.
The graphs on the right illustrates the placement at the 150th iteration (or the final iteration if it concluded before then) for each algorithm with the seed 0.

The observations and implications of Fig.~\ref{fig:individual} are as follows.
First, the solid plots for \textsf{SN} generally demonstrates superior performance compared to non-\textsf{SN}, validating the efficacy of the proposed method.
Some exception of the superiority arise with \textsf{FR}, which exhibits oscillations in the plot, likely due to excessive step sizes leading to overshooting.
Although we refrained from altering \textsf{FR} for fairness, adjusting the step size could enable the proposed method to achieve its intended performance.
In fact, the initial $f(X)$ of \textsf{SN-FR} is clearly small enough compared to the objective function values produced by \textsf{FR} alone, suggesting that the proposed method is yielding a good initial placement.
Additionally, the visualization results support the effectiveness of the proposed initial placement. In most cases, placements obtained with \textsf{SN} better represent the intended geometric arrangement compared to non-\textsf{SN} placements.

As a side note, regardless of \textsf{SN} or non-\textsf{SN}, the algorithms using \textsf{L-BFGS} consistently outperforms those using \textsf{FR}.
This finding is consistent with prior research~\cite{6183577}, though, regrettably, this technique remains relatively unknown in the field of graph drawing.
One of the aims of our paper is to further emphasize and popularize the use of \textsf{L-BFGS} in graph drawing, and these results provide a strong basis for this argument.

\begin{figure*}[btp]
    \centering
    \addtolength{\tabcolsep}{-0.5em}
    \begin{tabular}{ccccc}
        \multicolumn{5}{c}{\textbf{\texttt{cycle300}} $(\abs{V}=300, \abs{E}=300, \text{sparsity}=0.669\text{\%})$}     \\
        \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/cycle300.pdf}} &
        \makecell{\small{\textsf{FR}}                                                                                   \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/cycle300_FR.pdf}} &
        \makecell{\small{\textsf{L-BFGS}}                                                                               \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/cycle300_L_BFGS.pdf}} &
        \makecell{\small{\textsf{SN-FR}}                                                                                \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/cycle300_SN_FR.pdf}} &
        \makecell{\small{\textsf{SN-L-BFGS}}                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/cycle300_SN_L_BFGS.pdf}} \\
        \multicolumn{5}{c}{\textbf{\texttt{jagmesh1}} $(\abs{V}=936, \abs{E}=2664, \text{sparsity}=0.609\text{\%})$}    \\
        \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/jagmesh1.pdf}} &
        \makecell{\small{\textsf{FR}}                                                                                   \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/jagmesh1_FR.png}} &
        \makecell{\small{\textsf{L-BFGS}}                                                                               \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/jagmesh1_L_BFGS.png}} &
        \makecell{\small{\textsf{SN-FR}}                                                                                \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/jagmesh1_SN_FR.png}} &
        \makecell{\small{\textsf{SN-L-BFGS}}                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/jagmesh1_SN_L_BFGS.png}} \\
        \multicolumn{5}{c}{\textbf{\texttt{dwt\_1005}} $(\abs{V}=1005, \abs{E}=3808, \text{sparsity}=0.755\text{\%})$}  \\
        \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/dwt_1005.pdf}} &
        \makecell{\small{\textsf{FR}}                                                                                   \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_1005_FR.png}} &
        \makecell{\small{\textsf{L-BFGS}}                                                                               \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_1005_L_BFGS.png}} &
        \makecell{\small{\textsf{SN-FR}}                                                                                \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_1005_SN_FR.png}} &
        \makecell{\small{\textsf{SN-L-BFGS}}                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_1005_SN_L_BFGS.png}} \\
        \multicolumn{5}{c}{\textbf{\texttt{btree9}} $(\abs{V}=1023, \abs{E}=1022, \text{sparsity}=0.196\text{\%})$}     \\
        \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/btree9.pdf}}   &
        \makecell{\small{\textsf{FR}}                                                                                   \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/btree9_FR.png}} &
        \makecell{\small{\textsf{L-BFGS}}                                                                               \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/btree9_L_BFGS.png}} &
        \makecell{\small{\textsf{SN-FR}}                                                                                \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/btree9_SN_FR.png}} &
        \makecell{\small{\textsf{SN-L-BFGS}}                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/btree9_SN_L_BFGS.png}} \\
        \multicolumn{5}{c}{\textbf{\texttt{1138\_bus}} $(\abs{V}=1138, \abs{E}=1458, \text{sparsity}=0.225\text{\%})$}  \\
        \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/1138_bus.pdf}} &
        \makecell{\small{\textsf{FR}}                                                                                   \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/1138_bus_FR.pdf}} &
        \makecell{\small{\textsf{L-BFGS}}                                                                               \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/1138_bus_L_BFGS.pdf}} &
        \makecell{\small{\textsf{SN-FR}}                                                                                \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/1138_bus_SN_FR.pdf}} &
        \makecell{\small{\textsf{SN-L-BFGS}}                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/1138_bus_SN_L_BFGS.pdf}} \\
        \multicolumn{5}{c}{\textbf{\texttt{dwt\_2680}} $(\abs{V}=2680, \abs{E}=11173, \text{sparsity}=0.311\text{\%})$} \\
        \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/dwt_2680.pdf}} &
        \makecell{\small{\textsf{FR}}                                                                                   \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_2680_FR.pdf}} &
        \makecell{\small{\textsf{L-BFGS}}                                                                               \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_2680_L_BFGS.pdf}} &
        \makecell{\small{\textsf{SN-FR}}                                                                                \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_2680_SN_FR.pdf}} &
        \makecell{\small{\textsf{SN-L-BFGS}}                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/dwt_2680_SN_L_BFGS.pdf}} \\
        \multicolumn{5}{c}{\textbf{\texttt{3elt}} $(\abs{V}=4720, \abs{E}=13722, \text{sparsity}=0.123\text{\%})$}      \\
        \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/3elt.pdf}}     &
        \makecell{\small{\textsf{FR}}                                                                                   \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/3elt_FR.pdf}} &
        \makecell{\small{\textsf{L-BFGS}}                                                                               \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/3elt_L_BFGS.pdf}} &
        \makecell{\small{\textsf{SN-FR}}                                                                                \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/3elt_SN_FR.pdf}} &
        \makecell{\small{\textsf{SN-L-BFGS}}                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/viz/3elt_SN_L_BFGS.pdf}} \\
    \end{tabular}
    \caption{Numerical experiment results for various graphs.  Please refer Sec.~\ref{ssec:exprDetail} for details.}
    \label{fig:individual}
\end{figure*}

\section{Challenges of the Subspace Methods}\label{sec:challenges}

Here, we explain the rationale for why we took such a roundabout approach to solve the optimization problem~\eqref{eq:fr}.
As we have explained, we first transformed it into a discrete optimization problem~\eqref{eq:frApprox3} and then solved it using the subspace Newton direction.

Initially, we attempted a direct application of the subspace method, one of the general optimization methods, to the FR layout as described in Section~\ref{ssec:introNewton}.
However, we encountered several issues that made this approach challenging: the inaccuracy of the quadratic approximation and the ignorance of other vertex movements. We state these issues in Sec.~\ref{ssec:inaccuracy} and Sec.~\ref{ssec:ignorance}
Based on these challenges, we explain the rationale for our approach in Sec.~\ref{ssec:rationale}.

The important point is that these issues do not necessarily preclude the possibility of achieving global optimization with the subspace method.
Rather, we hope that an accurate understanding of these challenges will lead us to develop better optimization methods in the future.

\subsection{Introduction of Randomized Subspace Newton}\label{ssec:introRSN}

In this subsection, we introduce the Randomized Subspace Newton (RSN), which is NOT a proposed method but a concept that heavily inspired our proposed algorithm.
The RSN method is one of the subspace methods, and RSN and its variant have been proposed in the context of optimization problems~\cite{NEURIPS2019_bc6dc48b,
    fujiRandomizedSubspaceRegularized2022,
    cartisRandomisedSubspaceMethods2022,
    nozawaRandomizedSubspaceGradient2023,
    higuchiFastConvergenceSecondOrder2024}.

RSN focuses on a subspace of dimension $s$ randomly selected from the solution space by a projection matrix $S \in \bbR^{n \times s}$ and utilizes the exact Hessian matrix of size $s \times s$ defined on this subspace: $S^\top \nabla^2 f(x) S$.
At each iteration, RSN updates the solution by
\begin{equation*}
    x \gets x - S (S^\top \nabla^2 f(x) S)^{-1} S^\top \nabla f(x)
\end{equation*}
if $S^\top \nabla^2 f(x) S$ is non-singular.
Since $s \ll n$, the computational cost per iteration is significantly reduced when we disregard the cost of selecting the subspace.

The RSN method resembles the stochastic coordinate descent method, which updates only a subset of the variables at each iteration using gradient information.
The difference is that RSN uses the Hessian matrix to determine the update direction, bringing the method closer to the Newton method.

Moreover, recent studies have explored its application not only to convex optimization problems but also to non-convex optimization problems~\cite{fujiRandomizedSubspaceRegularized2022}, using a regularization term to ensure the convergence of the method.

In particular, our problem~\eqref{eq:fr} exhibits a natural affinity with the RSN method, as it inherently defines a subspace of the solution space $X$ for the FR layout: $x_i \in \bbR^2$ for all $v_i \in V$.
If take $S$ as a projection matrix that selects a random vertex $v_i$ from $V$, we can consider to apply the RSN method or its variant to the FR layout.

Despite of such high expectation for the subspace methods, we found that direct applying of the subspace method to the FR layout is quite challenging.
We take the following algorithm as a direct application of the subspace methods to the FR layout; Namely, we randomly select a vertex $v_i$, apply Newton's method or its regularized variant to $f_i$ using the gradient in Eq.~\eqref{eq:gradientFi} and its Hessian:
\begin{gather*}
    \nabla^2 f_i(x_i) = \sum_{j \neq i} \left(\frac{w_{i,j}\norm{x_i - x_j}}{k} - \frac{k^2}{\norm{x_i - x_j}^2}\right) I_d + \\
    \sum_{j \neq i} \left(\frac{w_{i,j}}{k \norm{x_i - x_j}} + \frac{2k^2}{\norm{x_i - x_j}^4}\right) (x_i - x_j)(x_i - x_j)^\top.
\end{gather*}
Then, we update the position of vertex $v_i$, and repeat this process until convergence.
However, this approach fails to work effectively in practice.
In the following, we explain the reasons behind this difficulty.

\subsection{Inaccuracy of quadratic approximation}\label{ssec:inaccuracy}

\begin{figure}[t]
    \centering
    \includegraphics[height=2.2cm]{whyRSNfail/whyRSNfail.pdf}
    \caption{
        The inaccurate quadratic approximation.
        Assume that the optimal placement of the graph is as shown on the left.
        For the red vertex on the right graph, its Newton direction is the red arrow, which is apparently a bad direction.
    }
    \label{fig:whyRSNfail}
\end{figure}

This subsection discusses the inaccuracy of quadratic approximation used in the Newton's method, particularly a specific issue arises when restricting the optimization to a subspace.

When we apply the Newton's method to a randomly selected vertex $v_i$, we approximate the energy function $f_i$ as a quadratic function $\overline{f}_i$ at $x_i$.
If this approximated function is convex, then the Newton's method update $v_i$ to the optimal solution of the approximated function.
Otherwise, by adding regularization terms, such as the cubic regularization~\cite{nesterovCubicRegularizationNewton2006}, we can update $v_i$ to a descent direction of $\overline{f}_i$.

Nevertheless, the update of $x_i$, which locally improves $\overline{f}_i$, does not necessarily improves the overall energy function $f$.

We show an example of this issue in Fig.~\ref{fig:whyRSNfail}.
Let a graph $G$ be as shown in the left (optimal), where $k$ and all positive edge weights $w_{i,j}$ are set to 1.
In a successful case such as in the middle (success), the subspace method works effectively, since the Newton direction defined on $f_i$ (blue arrow) leads to the improvement of the overall energy function $f$.

However, in the situation depicted on the right (failure), the Newton direction for $x_1$ (red arrow) is apparently a bad direction, leading to a significant deviation from the global optimal solution as it is.
The points of $G$ are set as
\begin{equation*}
    X = \begin{pmatrix}
        0 & -1 & -0.85  & -0.85  & 1 \\
        0 & 0  & +0.155 & -0.155 & 0
    \end{pmatrix},
\end{equation*}
and the Hessian $\nabla^2 f_1(x_1)$ is approximately
\begin{equation*}
    \begin{pmatrix}
        1.841 & 0     \\
        0     & 1.159
    \end{pmatrix},
\end{equation*}
which is positive definite and not ill-conditioned.
Despite of such a good property of the Hessian, the Newton direction for $x_1$ (red arrow) is a bad direction, since the repulsive force between $x_1$ and $x_2,x_3$ are too strong to diverge from the global optimal solution.
This kind of illness cannot be resolved by just modifying the Newton's method we use for $f_i$, and it is a inherent problem of the subspace method.

\subsection{Ignorance of other vertex movements}\label{ssec:ignorance}

\begin{figure}[t]
    \centering
    \includegraphics[height=2.2cm]{whyRSNfail2/whyRSNfail2.pdf}
    \caption{
        The ignorance of other vertex movements. Assume that the blue arrows show the forces to the vertices and the optimal movements in this situation. Nevertheless, the red vertex will only move a little by the subspace method.
    }
    \label{fig:whyRSNfail2}
\end{figure}

This subsection discusses another issue of the subspace method, which is the ignorance of other vertex movements when optimizing each vertex individually.
When optimizing for a vertex $v_i$, the subspace method treat all other vertices $v_j (j \neq i)$ as fixed. However, both the FR algorithm and the L-BFGS method do not, which represents a significant difference.

This is illustrated in Figure~\ref{fig:whyRSNfail2}.
Consider a subset of vertices in a mesh-like structure $G$, all vertices receive forces to the blue arrow directions and can minimize $f$ by moving in that direction.
In this setting, both the FR algorithm and the L-BFGS method progress the optimization without issue.
On the other hand, if we attempt to optimize only for the red vertex in the right graph, heavily influenced by its directly connected neighbors, $v_0$ barely moves.
As a result, the overall optimization barely advances, in contrast to the alignment of the blue arrows.
The same problem occurs for other vertices.

In this way, ignoring the direction of forces on other vertices, or more precisely, neglecting the values of $\pdv{f}{x_i}{x_j}$, is a major shortcoming of the subspace method.

\subsection{Rationale for the Proposed Method}\label{ssec:rationale}

As explained, we suspect that, while the subspace method is effective for reducing the ``twist'' in a rough sense, it is not suitable for optimizing the overall placement.

However, by converting the problem as stated in Section~\ref{ssec:reduction}, we can take this ignored interaction into account to some extent.
The advantage of problem~\eqref{eq:frApprox3} is that not only reduction of the computational complexity from $\order{\abs{V}^2}$ to $\order{\abs{E}}$, but also allowing us to more easily take account for vertex interactions and prevent them from becoming too close to each other, since the mapping $\pi$ is injection.
Thus, making the problem discrete brings the acceleration of each iteration as well as the high quality of the updated solution.

The important point is that, combined with some appropriate ideas, the subspace method can be very effective for the FR layout.
Focusing on optimizing each vertex individually is a natural and valid approach, and with further refinements, the subspace method holds the potential to efficiently yield global optimal solutions.

\section{Discussion} \label{sec:discussion}

In this section, we discuss the future directions of this research.
Firstly, the proposed method would be better to combine with the some conventional techniques such as Multilevel approach. We explain this in Sec.~\ref{sec:combination}.
Secondly, we explore the applications beyond the scope of graph drawing in Sec.~\ref{ssec:application}.
Finally, we conclude this paper in Sec.~\ref{sec:conclusion}.

\subsection{Combination with Other Techniques}\label{sec:combination}

This paper has demonstrated the effectiveness of the proposed method on relatively small-scale graphs, but it is believed that it can also be applied to more larger-scale problems.
For instance, the Scalable Force-Directed Placement (sfdp) of Graphviz~\cite{ellsonGraphvizOpenSource2002}, based on \cite{Hu2006EfficientHF}, employs a multilevel approach to accelerate processing for larger graphs by progressively coarsening vertices.
The result of sfdp shown in Fig.~\ref{fig:cycle} validate the utility of this technique.

The coarsening operation does not conflict with the proposed method, making it feasible to combine both approaches.
Specifically, by iteratively applying the proposed method to the entire coarsened graph or to groups of vertices consolidated through coarsening, it is possible to extend its applicability to larger-scale problems.
This approach is expected to yield faster and higher-quality solutions.
Addressing this integration is one of the challenges for future research.

In addition, the FR layout sometimes used not only in 2d but also in 3d~\cite{14738716211060306}. Although we have to modify some parts of the proposed algorithm to apply to 3d FR layout, such as the hexagonal lattice, its application would be not so difficult.

\subsection{Application to Other Problems}\label{ssec:application}

Although we proposed a subspace-based algorithm only for the optimization problem~\eqref{eq:fr}, we can see that the application of it is not necessarily limited to the FR layout alone.
In this section, we briefly discuss and explore the potential applicability of the subspace-based method to a wider range of problems.

In general, the optimization problem~\eqref{eq:fr} is more broadly treated as ``objective functions arising from graphs''~\cite{recht-wright}:
\begin{mini*}
    {X \in \bbR^{2 \times n}}
    {f(X) = \sum_{(i,j)\in E} f_{i,j}(x_i, x_j)+\lambda \sum_{i=1}^{n} \Omega_i(x_i),}
    {}
    {}
\end{mini*}
where $\Omega_i$ is a regularization term for the $i$-th vertex, and $\lambda$ is a regularization parameter.
The FR layout problem~\eqref{eq:fr} is obviously a special case of this problem class.

We can expect the subspace-based method to be effective for such problems, as shown in this study.
The authors of~\cite{recht-wright} claim that coordinate descent is an effective method for solving such problems. Given the similarity between the subspace-based method and coordinate descent as depicted in Sec.~\ref{ssec:introRSN}, this claim supports the potential of the subspace-based method for a wider range of problems.

For instance, we suspect that the subspace-based method can also be applied to the graph isomorphism problem.
The graph isomorphism problem is a well-known combinatorial optimization problem to determine whether two graphs $G_1,G_2$ are isomorphic, i.e., $G_1 \cong G_2$.
In fact, the graph isomorphism problem is closely related to the graph drawing. Drawing a graph in a way that reveals its symmetry is at least as difficult as the graph isomorphism problem~\cite{eades1984heuristic}. Indeed, if two graphs $G_1$ and $G_2$ can be drawn in the same way, it becomes evident that $G_1 \cong G_2$. See Fig.~\ref{fig:iso} for reference.
When we relax it to a continuous optimization problem on Riemannian manifolds as in~\cite{klusContinuousOptimizationMethods2023, klusContinuousOptimizationMethods2023}, we might be able to apply the subspace-based method to this problem as well.
Investigating the applicability of the subspace-based method to these problems constitutes one of the future research directions.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{iso/iso.pdf}
    \caption{
        Illustration of the relationship between graph drawing and graph isomorphism.
        If we can draw graphs $G_1$ and $G_2$ symmetrically, then it is clear that $G_1 \cong G_2$.
    }
    \label{fig:iso}
\end{figure}

\subsection{Conclusion} \label{sec:conclusion}

In this study, we proposed a new algorithm for the FR Layout that utilizes the subspace-based method.
To demonstrate its effectiveness, we conducted numerical experiments, which revealed that the proposed method is effective across a variety of graphs. For the research question posed in Sec.~\ref{sec:RQ}, we can answer that ``We can accelerate the optimization process for the FR layout by leveraging the inherent structure of the problem, especially with the subspace Newton direction''.

We conclude this paper expecting that the proposed method may advance graph drawing with FR layout and highlight the potential of the subspace-based method for addressing a wider range of graph-related optimization problems.

\section{Acknowledgment}

The author would like to express our sincere gratitude to PL Poirion and Andi Han for their insightful discussions, which have greatly inspired and influenced this research.

The author also thanks the developers of NetworkX and Graphviz. Their excellent work on the library has been a great help in conducting this research.

This work was partially supported by JSPS KAKENHI (23H03351,24K23853) and JST ERATO (JPMJER1903).

\ifthenelse{\boolean{isMain}}{
    \bibliographystyle{IEEEtran}
    \bibliography{FruchtermanReingoldByRandomSubspace}
}{}

\appendices

\section{Optimal Scaling}\label{sec:scaling}

When we optimize a placement for FR-layout with an initial placement obtained, scaling the initial placement at first can often yield better results than directly using the unmodified initial placement.
In this section, we explain how to find the optimal scaling factor that minimizes the energy function for a given placement.

Let us formulate the optimization problem for the scaling factor $c \in \bbR_{> 0}$. For an initial placement $X = (x_1, \dots, x_n)$, we rescale it as $x_i \gets c x_i$ for all $i$.
This problem is to minimize the energy function $\phi(c)$ defined as
\begin{align*}
    \phi(c) \defeq {} & \qty(\sum_{i < j} \frac{w_{ij} (cd_{ij})^3}{3k}) - k^2 \sum_{i < j} \log(c d_{ij})                                   \\
    \begin{split}
        = {} & c^3 \qty(\sum_{i < j} \frac{w_{ij} d_{ij}^3}{3k}) - k^2 n(n-1)\log(c) \\
             & - k^2 \sum_{i < j} \log(d_{ij}),
    \end{split} \\
    \phi'(c) = {}     & 3c^2 \qty(\sum_{i < j} \frac{w_{ij} d_{ij}^3}{3k}) - \frac{k^2 n(n-1)}{c},                                           \\
    \phi''(c) = {}    & 6c \qty(\sum_{i < j} \frac{w_{ij} d_{ij}^3}{3k}) + \frac{k^2 n(n-1)}{c^2}.
\end{align*}

The function $\phi(c)$ is convex, and we can find the optimal scaling factor $c$ by using Newton's method.
\orange{This algorithm achieves sufficient convergence within a few iterations}, and by pre-computing the coefficients of $\phi(c)$ with $w_{i,j} > 0$, the time complexity is just $\order{\abs{E}}$.

\section{Another approach based on the Proposed Method}\label{sec:anotherApproach}

In this section, we discuss alternative approaches to the problem~\eqref{eq:frApprox3} other than the proposed method.

First, within the framework of the subspace-based method, one might consider using only gradient descent, not the Newton direction.
However, we were unable to achieve satisfactory results with this approach in our preliminary experiments.
Given that the subspace dimension is as small as 2, employing the Newton direction seems considerably more efficient.

Secondly, we can also consider an approach without the subspace-based method.
This involves updating all $\abs{V}$ vertices simultaneously like the FR algorithm.
The idea is to iteratively move the points $\{x_i\}_{1 \leq i \leq \abs{V}} \subseteq Q$ on the hexagonal grid to arbitrary points $\{x_i^\mathrm{new}\}_{1 \leq i \leq \abs{V}} \subseteq \bbR^2$, and then assign all these $\abs{V}$ points to the nearest points in $Q$.
The assignment problem from the $\bbR^2$ to $Q$ can be formalized as finding the optimal mapping $\pi: V \to Q$ that minimizes the distance between the new positions $x_i^\mathrm{new}$ and the assigned points in $Q$, which can be formulated as:
\begin{mini*} {\pi: V \to Q} {\sum_{i=1}^{\abs{V}} \norm{x_i^\mathrm{new} - \pi(v_i)}^2} {} {} \addConstraint{\pi(v_i)}{\neq \pi(v_j),\quad}{\forall v_i, v_j \in V,(v_i \neq v_j)} \end{mini*}
This problem can be solved in $\order{\abs{V}^3}$ using methods such as the minimum-cost flow or Hungarian algorithm.
Additionally, we can obtain a heuristic solution in $\order{\abs{V} \log \abs{V}}$ by appropriately sorting ${x_i^\mathrm{new}}{1 \leq i \leq \abs{V}}$.

While we are not expecting as good performances as the proposed method from the approaches above, they offer certain advantages, such as simplifying implementation or avoiding random access to arrays.

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Hiroki Hamaguchi}
    Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Naoki Marumo}
    Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
\end{IEEEbiography}
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Akiko Takeda}
    Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
    Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan.
\end{IEEEbiography}

\end{document}