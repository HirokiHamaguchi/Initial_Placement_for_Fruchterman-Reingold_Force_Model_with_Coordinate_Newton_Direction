\documentclass[dvipdfmx,10pt,journal,compsoc]{IEEEtran}

\usepackage{hyperref}       % \href{URL}{text}
\hypersetup{colorlinks=true,linkcolor=blue,citecolor=blue,urlcolor=blue}

\usepackage[linesnumbered, ruled, vlined]{algorithm2e}    % \begin{algorithm2e}
\usepackage{amsmath}        % \begin{align*}
\usepackage{amssymb}        % \mathbb{A}
\usepackage{amsthm}         % \newtheorem
\usepackage{bm,bbm}         % \bm{A}, \bbm{1}
\usepackage{booktabs}       % \toprule, \midrule, \bottomrule
\usepackage{cleveref}       % \cref
\usepackage{enumitem}       % \begin{enumerate}[label=(\alph*)]
\usepackage{ifthen}         % \ifthenelse
\usepackage{lipsum}         % \lipsum
\usepackage{makecell}       % \makecell{L1\L2}
\usepackage{mathrsfs}       % \mathscr{A}
\usepackage{mathtools}      % \mathrlap
\usepackage{multirow}       % \multirow
\usepackage[numbers,sort&compress]{natbib}         % \citet
\usepackage{optidef}        % \begin{mini*}{x}{f(x)}{}{}
\usepackage{orcidlink}      % \orcidlink
\usepackage{physics}        % \qty, \norm, \abs
\usepackage{subcaption}     % \captionsetup
\usepackage{subfiles}       % \subfile{file}
\usepackage{thm-restate}    % \begin{restatable}{theorem}{thm}
\usepackage{tikz}           % \begin{tikzpicture}
\usepackage{xparse}         % \NewDocumentCommand
% \usepackage{calc}         % \setlength
% \usepackage{cancel}       % \cancel
% \usepackage{parskip}      % \setlength{\parskip}{0.5em}
% \usepackage{csvsimple}    % \csvautotabular
% \usepackage{diagbox}      % \diagbox
% \usepackage{dsfont}       % \mathds{1}
% \usepackage{epsfig}       % \epsfig
% \usepackage{fancybx}      % \ovalbox
% \usepackage{float}        % \begin{figure}[H]
% \usepackage{lipsum}       % \lipsum
% \usepackage{listings}     % \begin{lstlisting}
% \usepackage{multicol}     % \begin{multicols}{2}
% \usepackage{nicematrix}   % \begin{NiceMatrix}
% \usepackage{qcircuit}     % \Qcircuit
% \usepackage{siunitx}      % \SI{1}{\second}
% \usepackage{stfloats}     % \begin{figure*}
% \usepackage{subcaption}   % \begin{subfigure}
% \usepackage{ulem}         % \sout
% \usepackage{wrapfig}      % \begin{wrapfigure}
% \usepackage[all]{xy}      % \xymatrix
% \usepackage[dvipdfmx]{graphicx}
% \usepackage[square, sort, comma, numbers]{natbib}

\definecolor{cA}{HTML}{0072BD}
\definecolor{cB}{HTML}{EDB120}
\definecolor{cC}{HTML}{77AC30}
\definecolor{cD}{HTML}{D95319}

\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcommand{\green}[1]{\textcolor{green}{#1}}
\newcommand{\brown}[1]{\textcolor{brown}{#1}}
\newcommand{\black}[1]{\textcolor{black}{#1}}
\newcommand{\orange}[1]{\textcolor{orange}{#1}}
\newcommand{\st}{\text{ s.t. }}
\newcommand{\Img}[1]{\mathrm{Im}\qty(#1)}
\newcommand{\Ker}[1]{\mathrm{Ker}\qty(#1)}
\newcommand{\Supp}[1]{\mathrm{supp}\qty(#1)}
\newcommand{\Rank}[1]{\mathrm{rank}\qty(#1)}
\newcommand{\floor}[1]{\left\lfloor #1 \right\rfloor}
\newcommand{\ceil}[1]{\left\lceil #1 \right\rceil}
% C++ (https://tex.stackexchange.com/questions/4302/prettiest-way-to-typeset-c-cplusplus)
\newcommand{\Cpp}{C\nolinebreak[4]\hspace{-.05em}\raisebox{.4ex}{\relsize{-3}{\textbf{++}}}}
% https://tex.stackexchange.com/questions/28836/typesetting-the-define-equals-symbol
\newcommand{\defeq}{\coloneqq}
\newcommand{\eqdef}{\eqqcolon}
% https://tex.stackexchange.com/questions/5502/how-to-get-a-mid-binary-relation-that-grows
\newcommand{\relmiddle}[1]{\mathrel{}\middle#1\mathrel{}}

% https://tex.stackexchange.com/questions/564216/newcommand-for-each-letter
\ExplSyntaxOn
\NewDocumentCommand{\definealphabet}{mmmm}{
\int_step_inline:nnn{`#3}{`#4}{
\cs_new_protected:cpx{#1 \char_generate:nn{##1}{11}}{
\exp_not:N #2{\char_generate:nn{##1}{11}}}}}
\ExplSyntaxOff

% added by marumo
\newcommand{\memom}[1]{\color{blue}[#1]\color{black}}
\newcommand{\memoh}[1]{\color{red}[#1]\color{black}}

\definealphabet{bb}{\mathbb}{A}{Z}
\definealphabet{rm}{\mathrm}{A}{Z}
\definealphabet{cal}{\mathcal}{A}{Z}
\definealphabet{frak}{\mathfrak}{a}{z}
% \definealphabet{scr}{\mathscr}{A}{Z}
% \definealphabet{frak}{\mathfrak}{A}{Z}

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{remark}{Remark}
\newtheorem{example}{Example}
\newtheorem{assumption}{Assumption}

\crefname{equation}{Eq.}{Eqs.}
\crefname{figure}{Fig.}{Figs.}
\crefname{tabular}{Tab.}{Tabs.}
\crefname{section}{Sec.}{Secs.}
\crefname{subsection}{Sec.}{Secs.}
\crefname{algorithm}{Algorithm}{Algorithms}

% https://qiita.com/rityo_masu/items/efd44bc8f9229e014237
\allowdisplaybreaks[4]

\usetikzlibrary{
  3d,
  % fit,
  calc,
  math,
  matrix,
  patterns,
  backgrounds,
  arrows.meta,
  shapes.geometric,
  decorations.pathmorphing,
}

% \providecommand{\main}{.}
\newboolean{isMain}
\setboolean{isMain}{true}

\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}

\title{
  Initial Placement
  for Fruchterman--Reingold Force Model
  with Coordinate Newton Direction
}
\author{
  Hiroki Hamaguchi\,\orcidlink{0009-0005-7348-1356}
  Naoki Marumo\,\orcidlink{0000-0002-7372-4275}
  Akiko Takeda\,\orcidlink{0000-0002-8846-4496}
  % <-this % stops a space
}

\IEEEtitleabstractindextext{%
  \begin{abstract}
    Graph drawing is a fundamental task in information visualization, with the Fruchterman--Reingold (FR) force model being one of the most popular choices.
    We can interpret this visualization task as a continuous optimization problem, which can be solved using the FR algorithm, the original algorithm for this force model, or the L-BFGS algorithm, a quasi-Newton method.
    However, both algorithms suffer from twist problems and are computationally expensive per iteration, which makes achieving high-quality visualizations for large-scale graphs challenging.

    In this research, we propose a new initial placement based on the stochastic coordinate descent to accelerate the optimization process.
    We first reformulate the problem as a discrete optimization problem using a hexagonal lattice and then iteratively move a randomly selected vertex along the coordinate Newton direction.
    We can use the FR or L-BFGS algorithms to obtain the final placement.

    We demonstrate the effectiveness of our proposed approach through experiments, highlighting the potential of coordinate descent methods for graph drawing tasks.
    Additionally, we suggest combining our method with other graph drawing techniques for further improvement.
    We also discuss the relationship between our proposed method and broader graph-related applications.
  \end{abstract}
  \begin{IEEEkeywords}
    Graph Drawing,
    Optimization,
    Fruchterman--Reingold Algorithm,
    L-BFGS algorithm
  \end{IEEEkeywords}}
\maketitle

\section{Introduction}\label{sec:introduction}

\IEEEPARstart{G}{raph} is a mathematical structure representing pairwise relationships between objects, and graph drawing is a fundamental task in information visualization.
Indeed, numerous kinds of models and algorithms have been proposed, and among these, one of the most popular choices is force-directed graph drawing.

In force-directed graph drawing, the force model is composed of particles with forces acting between them.
The equilibrium of these forces is considered suitable for graph visualization, and algorithms aim to find this equilibrium state.
Among the force models~\cite{eades1984heuristic,kamadaAlgorithmDrawingGeneral1989}, the Fruchterman--Reingold (FR) force model~\cite{fruchtermanGraphDrawingForcedirected1991,kobourovSpringEmbeddersForce2012} is the most prominent one, regarded as flexible, intuitive, and simple.
It is widely used in various graph drawing libraries such as NetworkX~\cite{hagberg2008exploring}, Graphviz~\cite{ellsonGraphvizOpenSource2002}, and igraph~\cite{csardiIgraphSoftwarePackage2006}.

Contrary to the advantages above, the algorithms using this model face challenges that make producing high-quality visualizations for large-scale graphs challenging.
The most critical issue is that \emph{twist} slows down the simulation process.
The term twist refers to unnecessary folded and tangled structures in the visualized graph~\cite{veldhuizenDynamicMultilevelGraph2007,cheongSnapshotVisualizationComplex2018}.
The results by the FR and L-BFGS algorithms from a random initial placement shown in \cref{fig:fig1} highlight these twist issues.
Although the optimal graph structure is simple, the twists are likely to occur and weaken or diminish the forces, leading to stagnation and suboptimal visualization outcomes.
The FR algorithm is proposed alongside the force model and the most commonly used approach, but the results are excessively twisted.
The L-BFGS algorithm, a family of quasi-Newton methods, has been reported as a more practical approach for graph drawing~\cite{6183577} and achieves better results than the FR algorithm.
While this algorithm can partially address the twist problem, it sometimes requires many iterations. As shown in the figure, it may fail to achieve optimal visualization within a limited number of iterations.
Further, these algorithms suffer from high computational complexity when directly applied, $\order{\abs{V}^2}$ per iteration, where $\abs{V}$ is the number of vertices.

\begin{figure}[t]
  \centering
  \includegraphics[width=0.8\columnwidth]{fig1/fig1.pdf}
  \caption{
    Comparison of the algorithms for the \texttt{jagmesh1}.
  }
  \label{fig:fig1}
\end{figure}

Another approach to the twist problem is to provide initial placement in the pre-processing step.
Indeed, a pre-processing step with Simulated Annealing (SA) is also known to be effective~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016} since SA can avoid getting stuck in local optima and leads to a better visualization combined with the FR algorithm.
Regrettably, this work only uses a circle initial placement for unweighted graphs with a simple method, leaving significant room to improve the effectiveness and extend the applicability. Refer to \cref{ssec:preprocessing} for more details.

In this paper, we propose a new initial placement for the FR force model as depicted in \cref{fig:fig1}.
We provide an initial placement with fewer twists than random placement within a short time, accelerating the subsequent optimization process. We can use both FR and L-BFGS algorithms to obtain the final placement.
This work extends the applicability of the initial placement idea to larger-scale, weighted, and complicated structure graphs.
To achieve this, we optimize the position of vertices one by one with the coordinate Newton direction, leveraging the inherent structure and the sparsity of graphs.
We also demonstrate its effectiveness through various experiments.

The rest of this paper is organized as follows.
\Cref{sec:FR_ForceModel} introduces the FR force model.
\Cref{sec:relatedWorks} reviews the related works.
\Cref{sec:algorithm} proposes our initial placement algorithm.
\Cref{sec:experiment} shows the experimental results.
\Cref{sec:challenges} provides the rationale of our proposed algorithm.
Finally, \cref{sec:discussion} discusses and concludes the paper.

\section{Fruchterman--Reingold Force Model}\label{sec:FR_ForceModel}

\citet{fruchtermanGraphDrawingForcedirected1991} proposed the FR force model for graph drawing based on the physical analogy of the system of particles.
Through the simulation of these forces, the FR algorithm seeks the equilibrium positions.
In contrast to this ordinary approach, we minimize the energy, also known as the stress, to seek equilibrium.
This section reviews the model and clarifies the problem we solve.

\begin{figure}[t]
  \centering
  \includegraphics[height=5.5cm]{fr_layout/fr_layout.pdf}
  \caption{
    (Top) The illustration of the force model. Forces act on every pair of vertices.
    (Bottom) Forces $F_{i,j}^\mathrm{a}(d)$ and $F^\mathrm{r}(d)$ work between vertices $i$ and $j$. The equilibrium of them is achieved at $d = k/\sqrt[3]{w_{i,j}}$, which equals $k$ when $w_{i,j} = 1$.
  }
  \label{fig:frLayout}
\end{figure}

Let $G = (V, E)$ be a connected undirected graph with vertex set $V = \{1, \dots, n\}$ and edge set $E$.
Each edge $\{ i,j\} \in E$ has weight $w_{i,j}>0$.
For convenience, we set $w_{i,j}=0$ for $\{i, j\} \notin E$.

The FR force model assumes forces between vertices. For vertices $i$ and $j$ with a distance $d > 0$ between them, an attractive force $F_{i,j}^\mathrm{a}(d)$ and a repulsive force $F^\mathrm{r}(d)$ work as:
\begin{equation*}
  F_{i,j}^\mathrm{a}(d) \defeq \frac{w_{i,j} d^2}{k}, \quad F^\mathrm{r}(d) \defeq -\frac{k^2}{d},
\end{equation*}
where $k > 0$ is a constant parameter, often set to $1/\sqrt{n}$.
The scalar potential of these forces~\cite{6183577} is given by
\begin{align*}
  E^\mathrm{a}_{i,j}(d) & \defeq \int_{0}^{d} F_{i,j}^\mathrm{a}(r) \dd{r} = \frac{w_{i,j} d^3}{3k}, \\
  E^\mathrm{r}(d)       & \defeq \int_{\infty}^{d} F^\mathrm{r}(r) \dd{r} = -k^2\log{d},             \\
  E_{i,j}(d)            & \defeq E_{i,j}^\mathrm{a}(d) + E^\mathrm{r}(d).
\end{align*}
For simplicity, we define $E_{i,j}(0)=\infty$.
Thus, the problem is to minimize the energy with positions $X \defeq (x_1, \dots, x_n) \in \bbR^{2 \times n}$:
\begin{mini}
  {X \in \bbR^{2 \times n}}
  {f(X) \defeq \sum_{i<j} E_{i,j}(\norm{x_i - x_j}).}
  {\label{eq:fr}}
  {}
\end{mini}
The local minimum of $f$ yields the equilibrium positions since $\nabla f(X)$ corresponds to the forces. Refer to \cref{fig:frLayout} for the explanation.

As mentioned, we will only consider undirected connected graphs with non-negative weights.
Although some algorithms can handle directed unconnected graphs with negative weights, we do not focus on such cases.
For directed graphs, slight modifications of algorithms or converting graphs to undirected ones can be effective.
For unconnected graphs, algorithms can be applied to each connected component independently.
When negative weights are present, the optimization problem~\eqref{eq:fr} can be unbounded, but with non-negative weights and the connectivity of $G$, the problem is always bounded and solvable.

While the energy function $E_{i,j}$ is convex and minimized when $d = k/\sqrt[3]{w_{i,j}}$, a function $x_i \mapsto E_{i,j}(\norm{x_i - x_j})$ is non-convex for a fixed $x_j$. $\norm{\cdot}$ denote the Euclidean norm in $\bbR^2$. Additionally, $E_{i,j}$ is not Lipschitz continuous as it diverges when $d \to 0$. These properties highlight the difficulty of the problem.
Refer to \cref{fig:energy3d} for an illustration of $E_{i,j}$.

\begin{figure}[t]
  \centering
  \includegraphics[height=5.5cm]{energy_3d/energy_3d.png}
  \caption{Energy function $E_{i,j}(\norm{x_i - x_j})$ for $x_i=(x_{i,1},x_{i,2})$, $x_j=(0,0), w_{i,j} = 1$, and $k = 1$.}
  \label{fig:energy3d}
\end{figure}

\section{Related Works}\label{sec:relatedWorks}

We briefly introduce some critical related works.

\subsection{Algorithms for FR Force Model}

As mentioned in~\cref{sec:introduction}, we can use the FR and L-BFGS algorithms to visualize graphs with the FR force model. These algorithms solve the problem~\eqref{eq:fr}, and both can be applied to the initial placement we provide. For details, refer to \cref{sec:algToProb1}.

\subsection{Pre-Processing by Simulated Annealing}\label{ssec:preprocessing}

Let $Q^\mathrm{circle} \defeq \qty{(\cos(2\pi i/n), \sin(2\pi i/n)) \mid 1 \leq i \leq n}$ be the points on a unit circle in $\bbR^2$.
For an unweighted graph $G$, let $E_2$ be a set of vertex pairs with a shortest path distance equal to 2.
Let $\angle(a, b)$ denote the angle between the lines from the origin to the points $a$ and $b$, measured in the interval $(-\pi, \pi]$.
This study defines the problem for pre-processing as follows (we change some notations for consistency):
\begin{mini}
  {X \in \bbR^{2 \times n}}
  {\sum_{\{i,j\}\in E \cup E_2} \abs{\angle(x_i, x_j)},}
  {\label{eq:sa}}
  {}
  \addConstraint{x_i}{\in Q^\mathrm{circle} \quad}{\text{for $1 \leq i \leq n$}}
  \addConstraint{x_i}{\neq x_j \quad}{\text{for $1 \leq i < j \leq n$}.}
\end{mini}
The problem~\eqref{eq:sa} is a discrete optimization problem where the placement is limited to $Q^\mathrm{circle}$ and uses angles, not the function $f$.
Ref.~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016} obtain a faster and better visualization by setting the result of Simulated Annealing (SA) for the problem~\eqref{eq:sa} as an initial placement for the FR algorithm.

Still, the following limitations remain:
\begin{itemize}
  \item The target graphs are restricted to unweighted ones.
  \item The layout is confined to a simple circle, which could be ineffective for complex structure graphs.
  \item The only neighborhood in the SA is the random swapping of two vertices, making the optimization process inefficient for large-scale graphs.
  \item $\abs{E_2}$ could be $\Theta(n^2)$, unable to leverage the sparsity of graphs if it exists.
\end{itemize}
We are dealing with these limitations and can regard our study as an extension of this prior work.

\subsection{Graph Drawing by Stochastic Gradient Descent}\label{ssec:sgd}

When we regard graph drawing as an optimization problem, Stochastic Gradient Descent (SGD) for Kamada--Kawai (KK) layout~\cite{kamadaAlgorithmDrawingGeneral1989} is one of the most notable works~\cite{zhengGraphDrawingStochastic2019}.
In the KK layout, we regard $G$ as a complete graph and assign the energy function $E_{i,j}^{\mathrm{KK}}$ to all edges.
SGD in this context means to repeat randomly selecting an edge $\{ i,j \}$ and updating $x_i$ and $x_j$ with the gradient of $E_{i,j}^{\mathrm{KK}}$. This algorithm is known to be effective in solving various optimization problems.

Although applying SGD to the FR force model problem~\eqref{eq:fr} is straightforward, it is ineffective for this problem.
This is because the force model we consider assigns the same function $E_{i,j}(d)=-k^2\log{d}$ to all $\{i,j\}$ such that $w_{i,j}=0$. Optimizing $E_{i,j}$ only increases the distance between vertices $i$ and $j$, no matter how close they are to each other in the optimal solution. Thus, the gradient of $E_{i,j}$ is not informative enough to find the optimal solution, and we need to develop a new optimization method for the problem~\eqref{eq:fr}.

Still, the idea of randomly selecting an edge and updating its position is quite suggestive. Based on this idea, we propose to randomly select a vertex and update its position.

\subsection{Newton Direction and Coordinate Newton Direction}\label{ssec:introNewton}

Let us consider a strictly convex function $f\colon \bbR^n \to \bbR$ at $x_0$.
The Newton direction $d = -\nabla^2 f(x_0)^{-1} \nabla f(x_0)$ is an optimal direction for the second order approximation of $f$:
\begin{equation*}
  f(x_0) + \nabla f(x_0)^\top (x - x_0) + \frac{1}{2} (x - x_0)^\top \nabla^2 f(x_0) (x - x_0).
\end{equation*}
$x = x_0 + d$ is the minimizer of this approximation.
Note that the Hessian matrix $\nabla^2 f(x_0)$ is positive definite since $f$ is strictly convex.
Although the Newton direction provides a critical step in iterative methods, it requires the computation of the inverse Hessian $\nabla^2 f(x_0)^{-1} \in \bbR^{n \times n}$, posing a high computational cost for large-scale problems.

Still, we can leverage the concept of the Newton direction in a different manner, the coordinate Newton direction.
Instead of computing the inverse Hessian $\nabla^2 f(x_0)^{-1}$ in the entire variable space $\bbR^n$, we limit the variable $x$ to its coordinate block $x_i$ with fewer dimensions, and compute $\nabla^2 f_i(x_i)^{-1} \nabla f_i(x_i)$ where $f_i$ is a restricted function of $f$ to $x_i$.
Since the coordinate Newton direction computation is much cheaper than that of the Newton direction, we can repeat this procedure many times.
In general, this idea is known as stochastic coordinate descent~\cite{recht-wright} or Randomized Subspace Newton (RSN)~\cite{NEURIPS2019_bc6dc48b} in a broader context.

In particular, this coordinate Newton direction has an apparent natural affinity to the problem~\eqref{eq:fr} in \cref{sec:FR_ForceModel}.
We can compute the coordinate Newton direction by taking the position $x_i$ of the vertex $i$ as the coordinate block.
Although directly applying this idea to the problem~\eqref{eq:fr} is challenging, as we will discuss in \cref{sec:challenges}, we leverage this coordinate Newton direction to propose our algorithm.

\section{Proposed Algorithm}\label{sec:algorithm}

This section proposes a new initial placement algorithm for the problem~\eqref{eq:fr}.
We define the discrete optimization problem for initial placement and propose a new algorithm based on stochastic coordinate descent.

\subsection{Discrete Optimization Problem for Initial Placement}\label{ssec:reduction}

Even at the expense of accuracy, obtaining an approximate solution quickly is crucial for the initial placement.
To obtain it, we simplify the problem~\eqref{eq:fr} into a more manageable and well-behaved discrete optimization problem:
\begin{mini}
  {X \in \bbR^{2 \times n}}
  {\sum_{\{i,j\}\in E} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k},}
  {\label{eq:frApprox0}}
  {}
  \addConstraint{x_i}{\in Q^\mathrm{hex} \quad}{\text{for $1 \leq i \leq n$}}
  \addConstraint{x_i}{\neq x_j \quad}{\text{for $1 \leq i < j \leq n$}.}
\end{mini}
where
\begin{equation*}
  Q^\mathrm{hex} \defeq \left\{ \qty(q+\frac{1}{2}r, \frac{\sqrt{3}}{2}r) \relmiddle| q \in \bbZ, r \in \bbZ \right\}.
\end{equation*}
This section explains how this simplification is derived.

First, the problem \eqref{eq:fr} is equivalent to the following:
\begin{mini*}
  {X \in \bbR^{2 \times n}}
  {\sum_{\{i,j\}\in E} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k}- \sum_{i<j} k^2\log{\norm{x_i - x_j}}.}
  {}
  {}
\end{mini*}
This formalism separates the $\order{\abs{E}}$ terms from $E^\mathrm{a}_{i,j}$ and the $\order{\abs{V}^2}$ terms from $E^\mathrm{r}$.

Here, following previous research mentioned in \cref{ssec:preprocessing}, we fix the possible positions $x_i$ of each vertex $i$ to a discrete set of points $Q$, where $\abs{Q} \geq \abs{V}$. It means that we consider the following:
\begin{mini*}
  {X \in \bbR^{2 \times n}}
  {\sum_{\{i,j\}\in E} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k}- \sum_{i<j} k^2\log{\norm{x_i - x_j}},}
  {}
  {}
  \addConstraint{x_i}{\in Q \quad}{\text{for $1 \leq i \leq n$}}
  \addConstraint{x_i}{\neq x_j \quad}{\text{for $1 \leq i < j \leq n$}.}
\end{mini*}
The goal is to simplify the objective function and choose $Q$ appropriately to derive a well-simplified problem.

Due to the sparsity of many practical graphs, $\abs{E} \ll \abs{V}^2$ holds.
We want to leverage this sparsity for simplification.
To do that, we have to drop the second term that arose from the repulsive energy $E^r$:
\begin{equation*}
  -\sum_{i<j}k^2\log{\norm{x_i - x_j}}.
\end{equation*}
We impose a condition to $Q$ such that $\norm{q_i - q_j} \geq \epsilon$ for all $q_i,q_j \in Q (q_i \neq q_j)$.
In this case, the term above is negligible.
Since $E^\mathrm{r}(d)=-k^2\log{d}$ is a convex function such that it decreases monotonically concerning $d$, for sufficiently large $d$, the value of $-k^2\log{d}$ does not vary excessively. For too small $d$, we can prevent the divergence of the energy function by setting $\epsilon$.
Thus, under this condition, we drop the second term and derive the problem as:
\begin{mini}
  {X \in \bbR^{2 \times n}}
  {f^\mathrm{a}(X) \defeq \sum_{\{i,j\}\in E} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k},}
  {\label{eq:frApprox2}}
  {}
  \addConstraint{x_i}{\in Q \quad}{\text{for $1 \leq i \leq n$}}
  \addConstraint{x_i}{\neq x_j \quad}{\text{for $1 \leq i < j \leq n$}}
\end{mini}
This means that we skip to consider the $\order{\abs{V}^2}$ pairs by fixing the possible point placement in advance, reducing the computational complexity to $\order{\abs{E}}$ and thus offering significant speedup. See \cref{fig:pi} for a visual explanation.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{pi/pi.pdf}
  \caption{
    Concept of $Q$.
    The assignment from $V$ to a discrete point placement $Q$, especially a hexagonal lattice $Q^\mathrm{hex}$.
    Apparently, among $X_1, X_2, X_3$, the right one $X_3$ is the best placement for the problem~\eqref{eq:frApprox0}.
  }
  \label{fig:pi}
\end{figure}

We can consider various placements for the $Q$ above, including $Q^\mathrm{circle}$~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016}.
In this study, we adopt a hexagonal lattice $Q^\mathrm{hex}$~\cite{patelHexagonalGrids2013,s22145179}.
When minimizing the attraction energy $f^\mathrm{a}$, it is advantageous for the points to cluster as closely as possible. In this context, the hexagonal lattice is known for its densest packing structure in space with the least distance $\epsilon$ between points and offers computational simplicity. Thus, $Q^\mathrm{hex}$ is a suitable choice, and we have derived the problem~\eqref{eq:frApprox0}.

\subsection{Newton Direction for Discrete Optimization}\label{ssec:newtonDirection}

Next, we solve the discrete optimization problem~\eqref{eq:frApprox0}.
Although this problem is challenging to solve just using simple methods, the coordinate Newton direction of a randomly selected vertex $i$ provides significant insights, as mentioned in \cref{ssec:introNewton}. Therefore, we can provide an efficient solution to the problem.
Let the limited objective function $f^{\mathrm{a}}_i(x_i)$ be
\begin{equation*}
  f^{\mathrm{a}}_i(x_i) \defeq \sum_{j \neq i} \frac{w_{i,j}\norm{x_i - x_j}^3}{3k}.
\end{equation*}
Its gradient and Hessian matrix are
\begin{align*}
  \nabla f^{\mathrm{a}}_i(x_i)   & = \sum_{j \neq i} \frac{w_{i,j}\norm{x_i - x_j}}{k} (x_i - x_j),                     \\
  \nabla^2 f^{\mathrm{a}}_i(x_i) & = \sum_{j \neq i} \frac{w_{i,j}\norm{x_i - x_j}}{k} \mqty(1                      & 0 \\0&1) \\
                                 & + \sum_{j \neq i} \frac{w_{i,j}}{k\norm{x_i - x_j}} (x_i - x_j)(x_i - x_j)^\top.
\end{align*}
This means $f^{\mathrm{a}}_i$ is strictly convex, assuring the Hessian matrix $\nabla^2 f^{\mathrm{a}}_i(x_i)$ is positive definite. This is a large difference from the functions $f_i(x_i)$ in \cref{eq:fi} and $f^{\mathrm{a}}(X)$ in the problem~\eqref{eq:frApprox2}, which are non-convex.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{hex/hex.pdf}
  \caption{One iteration of the proposed algorithm. Step1. Compute the coordinate Newton direction for a randomly selected vertex (blue). Step2. Decide $x_i^\mathrm{new}$ by rounding the direction and adding a random vector. Step3. Move the vertex and swap the vertices if there is a collision. In this case, swap blue and green vertices.}
  \label{fig:hex}
\end{figure}

The ordinary updated rule with the coordinate Newton direction is
\begin{equation*}
  x_i^\mathrm{new} \gets x_i - \nabla^2 f^{\mathrm{a}}_i(x_i)^{-1} \nabla f^{\mathrm{a}}_i(x_i).
\end{equation*}
$x_i^\mathrm{new}$ may not be in the hexagonal lattice $Q^\mathrm{hex}$ in the problem~\eqref{eq:frApprox0}.
Thus, we must round this value to the nearest point in $Q^\mathrm{hex}$.
We also empirically found that adding a random noise vector to the Newton direction is effective for the optimization process, a strategy similar to the SA in \cref{ssec:preprocessing}. This randomness can help to escape from local minima and to explore the solution space more effectively.
In conclusion, the updated rule for the vertex $i$ is
\begin{equation*}
  x_i^\mathrm{new} \gets \mathrm{round}\qty(x_i - \nabla^2 f^{\mathrm{a}}_i(x_i)^{-1} \nabla f^{\mathrm{a}}_i(x_i) + t \cdot \text{rand}),
\end{equation*}
where $\mathrm{round}(\hat{x})$ denotes the operation assigning $\hat{x}$ to the nearest point in the hexagonal lattice $Q^\mathrm{hex}$, $\mathrm{rand}$ is a random vector with a unit norm, and $t$ is a parameter controlling the randomness converges to 0.

If there is a vertex $j$ such that $x_j = x_i^\mathrm{new}$, we swap the positions $x_i$ and $x_j$ to satisfy the condition $x_i \neq x_j$.
Otherwise, we just update $x_i$ to $x_i^\mathrm{new}$. Refer to \cref{fig:hex} for a visual explanation.
Repeating this procedure solves the problem~\eqref{eq:frApprox0}.

\subsection{Optimal Scaling}\label{ssec:optimalScaling}

Finally, the obtained solution could be too small or too large since we did not care about the scale $\epsilon$.
Thus, as the final step, we rescale the placement to obtain the initial placement.
This subsection explains how to find the optimal scaling factor $c^*$ that minimizes $f$ for a given placement.

Let us formulate the optimization problem for the scaling factor $c > 0$.
For an initial placement $X = (x_1, \dots, x_n)$, we scale it as $x_i \gets c x_i$ for all $i$.
This problem is to minimize the energy function $\phi(c)$ defined by
\begin{align*}
  \phi(c) \defeq & \qty(\sum_{\{i,j\} \in E} \frac{w_{i,j} (c \norm{x_i - x_j})^3}{3k}) - k^2 \sum_{i < j} \log(c \norm{x_i - x_j}) \\
  \phi'(c) =     & 3c^2 \qty(\sum_{\{i,j\} \in E} \frac{w_{i,j} \norm{x_i - x_j}^3}{3k}) - \frac{k^2 n(n-1)}{2c}.
\end{align*}
The function $\phi(c)$ is convex, and the optimal scaling factor $c^*$ satisfies $\phi'(c^*) = 0$, which yields
\begin{equation}\label{eq:scaling}
  c^* = \qty(\frac{k^2 n(n-1)}{2 \sum_{\{i,j\} \in E} \frac{w_{i,j} \norm{x_i - x_j}^3}{k}})^{1/3}.
\end{equation}
This value can be computed in $\order{\abs{E}}$ complexity, enabling us to obtain a better initial placement for the problem~\eqref{eq:fr}.

Notably, the optimal solution to the problem~\eqref{eq:frApprox0} is invariant under scaling. Thus, we can select any $\epsilon$ to define the hexagonal lattice $Q^\mathrm{hex}$ as far as we scale by $c^*$.

\subsection{Pseudo Code}\label{ssec:pseudoCode}

We presented the overall framework of the proposed method in \cref{alg:proposed}.
The proposed algorithm is not a complete solution to the problem~\eqref{eq:fr}; this only provides an initial placement for the FR or L-BFGS algorithms.

\begin{algorithm}[ht]
  \caption{Proposed algorithm as initial placement}
  \label{alg:proposed}
  \KwIn{Graph $G = (V, E)$, Weight $(w_{i,j})_{\qty{i,j} \in E}$, Parameters $N_\mathrm{iter}^\mathsf{CN}\in\bbN,$ $t_0>0$}
  \KwOut{Initial placement $X = (x_1, \dots, x_n)$}
  $t \gets t_0$\;
  Sample $x_i \in Q$ for all $i \in V$ without replacement\;
  \For{$m \gets 0$ \KwTo $N_\mathrm{iter}^\mathsf{CN}$}{
  Select vertex $i \in V$ randomly\;
  $x_i^\mathrm{new} \gets \mathrm{round}(x_i - \nabla^2 f_i(x_i)^{-1} \nabla f_i(x_i) + t \cdot \mathrm{rand})$\;
    \If{$\exists j \in V \st x_j = x_i^\mathrm{new}$}{
      Swap $x_i$ and $x_j$\;
    } \Else{
      $x_i \gets x_i^\mathrm{new}$\;
    }
    $t \gets t - t_0 / N_\mathrm{iter}^\mathsf{CN}$\;
    }
  $x_i \gets c^* x_i$ for all $i \in V$ with $c^*$ by \cref{eq:scaling}\;
    \Return $X$
\end{algorithm}

\subsection{Alternative Approach}\label{ssec:alternative}

To end this section, we explain an alternative approach to this algorithm. We can also optimize by updating all vertices simultaneously, not one by one.
It means moving all the points $\{x_i\}_{i \in V}$ to arbitrary points and then assigning all these $\abs{V}$ points to the nearest points.
If appropriately defined, we can solve the assignment problem by minimum-cost flow or Hungarian algorithm with $\order{\abs{V}^3}$ complexity.
Additionally, a heuristic solution can be obtained in $\order{\abs{V} \log \abs{V}}$ by appropriately sorting.

They offer advantages such as simplified implementation or avoiding random access to arrays.

\section{Numerical Experiment} \label{sec:experiment}

In this section, we evaluate the proposed algorithm using various numerical experiments.
We also confirm that the proposed algorithm extends the applicability of the pre-processing step in Ref.~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016}, as mentioned in \cref{sec:introduction}.

\subsection{Experimental Setup}\label{ssec:setup}

We conducted all numerical experiments in this section using \Cpp17 compiled by GCC 10.5.0 on a laptop computer powered by Intel(R) Core(TM) i7-10510U CPU with 16 GB RAM.

For the FR and L-BFGS algorithms, we referenced NetworkX version 3.3~\cite{hagberg2008exploring}, SciPy 1.14.1 ~\cite{2020SciPy-NMeth}, and C++ L-BFGS~\cite{qiuYixuanLBFGSpp2024,okazakiChokkanLiblbfgs2024}. In particular, we used almost the same parameters as NetworkX's \textsf{spring\_layout} for the FR algorithm.
We also referenced the open-source code of the hexagonal grid from~\cite{patelHexagonalGrids2013} and Graphviz version 2.43.0~\cite{ellsonGraphvizOpenSource2002}.

We used the $3 \times 2$ algorithms.
As an initial placement, we used
\begin{itemize}
  \item Random initialization (no prefix),
  \item The circle initialization obtained by Simulated Annealing (\textsf{SA-})~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016},
  \item The proposed initialization obtained with coordinate Newton direction (\textsf{CN-}).
\end{itemize}
As an algorithm to solve the problem~\eqref{eq:fr}, we used
\begin{itemize}
  \item FR algorithm (\textsf{FR}),
  \item L-BFGS algorithm (\textsf{L-BFGS}).
\end{itemize}

As parameters, we used $N_\mathrm{iter}^\mathsf{CN} = 2 \abs{V}^3 / \abs{E},$ $t_0=1.5$ for~\cref{alg:proposed}, and we also set the total number of iterations of Simulated Annealing (\textsf{SA}) in~\cref{ssec:preprocessing} to the same value.
Since the amortized time complexity per iteration of \cref{alg:proposed} is $\order{\abs{E} / \abs{V}}$, we can roughly expect that the computational time of the proposed algorithm is $\order{2\abs{V}^2}$, equivalent to a few iterations of the FR or L-BFGS algorithm.
All the codes are available at GitHub~\cite{ThisPaperGitHub}.

\begin{figure*}[btp]
  \centering
  \addtolength{\tabcolsep}{-0.5em}
  \begin{tabular}{cccccc}
    \multicolumn{6}{c}{\textbf{\texttt{cycle300}} $(\abs{V}=300, \abs{E}=300, \text{sparsity}=0.669\text{\%})$ \quad Figures are at 150 iterations.}     \\
    \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/cycle300.pdf}} &
    \makecell{\small{\textsf{FR}}                                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/cycle300_FR.png}} &
    \makecell{\small{\textsf{L-BFGS}}                                                                                                                    \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/cycle300_L-BFGS.png}} &
    \makecell{\small{\textsf{\textbf{CN}-FR}}                                                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/cycle300_CN-FR.png}} &
    \makecell{\small{\textsf{\textbf{CN}-L-BFGS}}                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/cycle300_CN-L-BFGS.png}} &
    \makecell{\small{\textsf{BEST}}                                                                                                                      \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/opt_cycle300.png}} \\

    \multicolumn{6}{c}{\textbf{\texttt{jagmesh1}} $(\abs{V}=936, \abs{E}=2664, \text{sparsity}=0.609\text{\%})$ \quad Figures are at 50 iterations.}     \\
    \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/jagmesh1.pdf}} &
    \makecell{\small{\textsf{FR}}                                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/jagmesh1_FR.png}} &
    \makecell{\small{\textsf{L-BFGS}}                                                                                                                    \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/jagmesh1_L-BFGS.png}} &
    \makecell{\small{\textsf{\textbf{CN}-FR}}                                                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/jagmesh1_CN-FR.png}} &
    \makecell{\small{\textsf{\textbf{CN}-L-BFGS}}                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/jagmesh1_CN-L-BFGS.png}} &
    \makecell{\small{\textsf{BEST}}                                                                                                                      \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/opt_jagmesh1.png}} \\

    \multicolumn{6}{c}{\textbf{\texttt{dwt\_1005}} $(\abs{V}=1005, \abs{E}=3808, \text{sparsity}=0.755\text{\%})$ \quad Figures are at 100 iterations.}  \\
    \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/dwt_1005.pdf}} &
    \makecell{\small{\textsf{FR}}                                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_1005_FR.png}} &
    \makecell{\small{\textsf{L-BFGS}}                                                                                                                    \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_1005_L-BFGS.png}} &
    \makecell{\small{\textsf{\textbf{CN}-FR}}                                                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_1005_CN-FR.png}} &
    \makecell{\small{\textsf{\textbf{CN}-L-BFGS}}                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_1005_CN-L-BFGS.png}} &
    \makecell{\small{\textsf{BEST}}                                                                                                                      \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/opt_dwt_1005.png}} \\

    \multicolumn{6}{c}{\textbf{\texttt{btree9}} $(\abs{V}=1023, \abs{E}=1022, \text{sparsity}=0.196\text{\%})$ \quad Figures are at 150 iterations.}     \\
    \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/btree9.pdf}}   &
    \makecell{\small{\textsf{FR}}                                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/btree9_FR.png}} &
    \makecell{\small{\textsf{L-BFGS}}                                                                                                                    \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/btree9_L-BFGS.png}} &
    \makecell{\small{\textsf{\textbf{CN}-FR}}                                                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/btree9_CN-FR.png}} &
    \makecell{\small{\textsf{\textbf{CN}-L-BFGS}}                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/btree9_CN-L-BFGS.png}} &
    \makecell{\small{\textsf{BEST}}                                                                                                                      \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/opt_btree9.png}} \\

    \multicolumn{6}{c}{\textbf{\texttt{1138\_bus}} $(\abs{V}=1138, \abs{E}=1458, \text{sparsity}=0.225\text{\%})$ \quad Figures are at 150 iterations.}  \\
    \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/1138_bus.pdf}} &
    \makecell{\small{\textsf{FR}}                                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/1138_bus_FR.png}} &
    \makecell{\small{\textsf{L-BFGS}}                                                                                                                    \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/1138_bus_L-BFGS.png}} &
    \makecell{\small{\textsf{\textbf{CN}-FR}}                                                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/1138_bus_CN-FR.png}} &
    \makecell{\small{\textsf{\textbf{CN}-L-BFGS}}                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/1138_bus_CN-L-BFGS.png}} &
    \makecell{\small{\textsf{BEST}}                                                                                                                      \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/opt_1138_bus.png}} \\

    \multicolumn{6}{c}{\textbf{\texttt{dwt\_2680}} $(\abs{V}=2680, \abs{E}=11173, \text{sparsity}=0.311\text{\%})$ \quad Figures are at 150 iterations.} \\
    \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/dwt_2680.pdf}} &
    \makecell{\small{\textsf{FR}}                                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_2680_FR.png}} &
    \makecell{\small{\textsf{L-BFGS}}                                                                                                                    \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_2680_L-BFGS.png}} &
    \makecell{\small{\textsf{\textbf{CN}-FR}}                                                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_2680_CN-FR.png}} &
    \makecell{\small{\textsf{\textbf{CN}-L-BFGS}}                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/dwt_2680_CN-L-BFGS.png}} &
    \makecell{\small{\textsf{BEST}}                                                                                                                      \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/opt_dwt_2680.png}} \\

    \multicolumn{6}{c}{\textbf{\texttt{3elt}} $(\abs{V}=4720, \abs{E}=13722, \text{sparsity}=0.123\text{\%})$ \quad Figures are at 150 iterations.}      \\
    \raisebox{-.5\height}{\includegraphics[width=0.55\columnwidth]{individual/plot/3elt.pdf}}     &
    \makecell{\small{\textsf{FR}}                                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/3elt_FR.png}} &
    \makecell{\small{\textsf{L-BFGS}}                                                                                                                    \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/3elt_L-BFGS.png}} &
    \makecell{\small{\textsf{\textbf{CN}-FR}}                                                                                                            \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/3elt_CN-FR.png}} &
    \makecell{\small{\textsf{\textbf{CN}-L-BFGS}}                                                                                                        \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/3elt_CN-L-BFGS.png}} &
    \makecell{\small{\textsf{BEST}}                                                                                                                      \\[-0.2em]\includegraphics[width=0.27\columnwidth]{individual/vis/opt_3elt.png}} \\
  \end{tabular}
  \caption{Numerical experiment results for various graphs.  Please refer to \cref{ssec:exprDetail} for details.}
  \label{fig:individual}
\end{figure*}

\subsection{detailed Experiment}\label{ssec:exprDetail}

We conducted a detailed experiment to investigate the behavior of the proposed algorithm in detail. \Cref{fig:individual} shows the results.

The experiment details are as follows.
We fix the maximum number of iterations of the FR and L-BFGS algorithms to $N_\mathrm{iter}^\mathsf{FR}$ and $N_\mathrm{iter}^{\mathsf{L-BFGS}}$, respectively.
We set $N_\mathrm{iter}^\mathsf{FR} = N_\mathrm{iter}^\mathsf{L-BFGS} = 200$ in this experiment.
We tested with 7 graphs: \texttt{cycle300}, \texttt{jagmesh1}, \texttt{dwt\_1005}, \texttt{btree9}, \texttt{1138\_bus}, \texttt{dwt\_2680}, and \texttt{3elt}. \texttt{cycle300} is a cycle graph with 300 vertices, and \texttt{btree9} is a perfect binary tree with $2^{9+1}-1=1023$ vertices. Other graphs are from Sparse Matrix Collection~\cite{davis2011university}, and these choices are based on the experiments conducted in Ref.~\cite{zhengGraphDrawingStochastic2019}. Thus, although all the graphs are quite sparse so that $\abs{E} / (\abs{V}(V-1)/2)$ is less than 1\%, this is not an arbitrary choice. The important graphs often have such a sparsity.

We first explain what \cref{fig:individual} represents.
The plots on the left illustrate the objective function values $f(X)$ on the vertical axis versus execution time on the horizontal axis, using ten trials for each algorithm.
Faint crosses represent the results with random initialization, while faint circles represent the results of \textsf{CN}, plotted every ten iterations for each trial.
The solid and dashed lines represent the average of these values across the ten trials for each algorithm.
If one of the trials terminated before reaching $N_\mathrm{iter}^\mathsf{FR}$-th or $N_\mathrm{iter}^\mathsf{L-BFGS}$-th iterations, we plotted up to the minimum trial count achieved across all trials. Each trial was conducted with a different seed.
The graphs on the right are at the iteration in which the most significant difference appeared among \{50,100,150\}-th iterations (or at the last iteration if it ended earlier), using seed 1. We obtained the ``BEST'' column by running at most 500 iterations of \textsf{CN-L-BFGS}.
The colors of the vertices in the graphs presented in this section are assigned according to vertex indices using a color map that provides a gradient from blue to red.

The observations and implications of \cref{fig:individual} are as follows.
First, the solid plots for \textsf{CN} generally demonstrate superior performance compared to non-\textsf{CN}, validating the efficacy of the proposed method.
Some exceptions of the superiority arise with \textsf{FR}, which exhibits oscillations in the plot, likely due to excessive stepsizes leading to overshooting.
Although we refrained from altering \textsf{FR} for fairness, adjusting the stepsize (or using adaptive stepsize) could enable the proposed method to achieve its intended performance.
Still, the initial $f(X)$ of \textsf{CN-FR} is small enough compared to the objective function values produced by \textsf{FR} alone, suggesting that the proposed method is yielding a good initial placement.
Additionally, the visualization results support the effectiveness of the proposed initial placement. In most cases, placements obtained with \textsf{CN} better represent the intended geometric arrangement shown in the ``BEST''.

As a side note, regardless of \textsf{CN} or non-\textsf{CN}, the algorithms using \textsf{L-BFGS} consistently outperform those using \textsf{FR}.
This finding is consistent with prior research~\cite{6183577}, though regrettably, this technique remains relatively unknown in graph drawing.
One of the aims of our paper is to emphasize further and popularize the use of the L-BFGS algorithm in graph drawing, and these results provide substantial proof for this argument.

\subsection{Comparison with Other Initializations}\label{ssec:exprAll}

\begin{figure*}[t]
  \centering
  \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{overall/plot/diff_FR_50.pdf}
  \end{minipage}
  \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{overall/plot/diff_L-BFGS_50.pdf}
  \end{minipage}
  \caption{
    Comparison of the proposed initialization (\textsf{CN}) with random initialization (no prefix).
    In almost all cases, the difference is negative, meaning the proposed algorithm performed faster and better than random initialization.
  }
  \label{fig:overall}
\end{figure*}
\begin{figure*}[!t]
  \centering
  \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{circle/plot/diff_FR_50.pdf}
  \end{minipage}
  \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[width=\columnwidth]{circle/plot/diff_L-BFGS_50.pdf}
  \end{minipage}
  \caption{
    Comparison of the proposed initialization (\textsf{CN}) with circle initialization (\textsf{SA}) in Ref.~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016}.
    In most cases, the proposed algorithm performed better than the circle initialization.
  }
  \label{fig:diff}
\end{figure*}
\begin{figure*}[!t]
  \centering
  \begin{minipage}{\columnwidth}
    \centering
    \begin{minipage}{0.32\columnwidth}
      \includegraphics[height=2.8cm]{overall/vis/Spectro_10NN_CN-L-BFGS_50_first.png}
    \end{minipage}
    \begin{minipage}{0.32\columnwidth}
      \includegraphics[height=2.8cm]{overall/vis/Spectro_10NN_CN-L-BFGS_50_last.png}
    \end{minipage}
    \begin{minipage}{0.32\columnwidth}
      \includegraphics[height=2.8cm]{overall/vis/Spectro_10NN_CN-L-BFGS_50_final.png}
    \end{minipage}
    \caption{
      An example of a case where the proposed algorithm performed worse than random initialization.
      Left: Initial placement by \textsf{CN}.
      Middle and Right: 50th and 200th iteration of \textsf{CN-L-BFGS}.
    }
    \label{fig:proposeIsBad}
  \end{minipage}
  \begin{minipage}{\columnwidth}
    \centering
    \begin{minipage}{0.4\columnwidth}
      \centering
      \includegraphics[height=2.8cm]{circle/vis/dense_CN-L-BFGS_10.png}
    \end{minipage}
    \begin{minipage}{0.4\columnwidth}
      \centering
      \includegraphics[height=2.8cm]{circle/vis/dense_SA-L-BFGS_10.png}
    \end{minipage}
    \caption{
      An example of a case where $w_{i,j} \notin \qty{0,1}$.
      This result shows the proposed algorithm is extending the applicability of the pre-processing step.
      Left: the result of \textsf{CN}. Separated by colors.
      Right: the result of \textsf{SA}. No separation.
    }
    \label{fig:weightedDense}
  \end{minipage}
\end{figure*}
\begin{figure*}[!t]
  \centering
  \begin{tabular}{cccccc}
    \toprule
     & \multicolumn{2}{c}{\texttt{dwt\_992} (\textsf{SA} better)}
     & \multicolumn{2}{c}{\texttt{collins\_15NN} (\textsf{CN} better)}                                                                                 \\
     & initial placement                                                                       & 50th iteration & initial placement & 50th iteration & \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5}
    \rotatebox{90}{\textsf{SA} (Ref.~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016})}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/dwt_992_SA-L-BFGS_50_first.png}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/dwt_992_SA-L-BFGS_50_last.png}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/collins_15NN_SA-L-BFGS_50_first.png}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/collins_15NN_SA-L-BFGS_50_last.png}                                                          \\
    \addlinespace
    \rotatebox{90}{\textsf{CN} (proposed)}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/dwt_992_CN-L-BFGS_50_first.png}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/dwt_992_CN-L-BFGS_50_last.png}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/collins_15NN_CN-L-BFGS_50_first.png}
     & \includegraphics[width=0.3\columnwidth]{circle/vis/collins_15NN_CN-L-BFGS_50_last.png}                                                          \\
    \bottomrule
  \end{tabular}
  \caption{Visualization results showing initial and 50th iteration placements for \texttt{dwt\_992} and \texttt{collins\_15NN}.}
  \label{fig:CN_vs_SA}
\end{figure*}

Next, we conducted exhaustive experiments to evaluate the performance of the proposed algorithm (\textsf{CN}) compared to random initialization (no prefix) and circle initialization (\textsf{SA}) with various graphs.

Let $W = (w_{i,j}) \in \bbR_{\geq 0}^{n \times n}$ be an adjacency matrix of a graph $G$.
Since we only consider undirected connected graphs with non-negative weights, $W$ should satisfy the following conditions:
\begin{equation*}
  W \in \bbR_{\geq 0}^{n \times n}, \quad W = W^\top, \quad \text{and \quad $G$ is connected}.
\end{equation*}
We used matrices from Sparse Matrix Collection~\cite{davis2011university} satisfying the above condition with $\abs{V} \leq 1000$ as the adjacency matrix $W$, in total 124 graphs.
For fairness, when we compare with \textsf{SA}, we converted the graph to an unweighted one. We set weights $w_{i,j}$ to $1$ if $w_{i,j} > 0$; otherwise, we set it to $0$.

We set $N_\mathrm{iter}^\mathsf{FR}$ and $N_\mathrm{iter}^{\mathsf{L-BFGS}}$ as $50$, the default parameter of NetworkX~\cite{hagberg2008exploring}.
For the \textsf{CN} algorithms compared with random initialization,
We set them as 45 since it contains the pre-processing step. We can roughly expect that the computational time is equivalent to a few iterations of FR or L-BFGS algorithms, as mentioned in \cref{ssec:setup}.

The results are shown in \cref{fig:overall} and \cref{fig:diff}.
In almost all cases, the proposed algorithm performed better than random or circle initialization.
A few cases where the proposed algorithm performed worse than other methods.
As for random initialization, one of such cases is \texttt{Spectro\_10NN
} shown in \cref{fig:proposeIsBad}. The proposed algorithm might fail to resolve the twist in the initial placement, shown in the figure, leading to a worse result. Still, the average performance for this case was almost the same as that of the random initialization.
As for circle initialization, we showed examples in \cref{fig:CN_vs_SA}.
The proposed algorithm outperforms in \texttt{collins\_15NN} and underperforms in \texttt{dwt\_992}.

The interpretation of the results is as follows.
First, this result strongly supports the effectiveness of the proposed algorithm. It successfully untangles the twists, leading to better outcomes with faster convergence. Even when the proposed algorithm performed worse, the difference was insignificant in almost all cases.
Secondly, the choice of initial placement can lead to advantages or disadvantages depending on the optimal placement.
In particular, since the optimal shape of \texttt{collins\_15NN} is a linear shape, which differs from a circle, \texttt{SA}'s performance can be worse than the proposed algorithm.
Such difference likely contributed to the advantage of the proposed algorithm.
Although the proposed method uses a hexagonal lattice $Q^\mathrm{hex}$, it can be adapted to other fixed placement $Q$.
If the shape of the optimal solution is roughly known in advance, selecting a different $Q$, such as a $Q^\mathrm{circle}$, could further enhance the performance of the proposed algorithm.

Additionally, although this is a case not considered in Ref.~\cite{ghassemitoosiSimulatedAnnealingPreProcessing2016}, we also conducted experiments with \textsf{CN} and \textsf{SA} for a case where the edge weight $w_{i,j}$ is not necessarily in $\qty{0,1}$.
We made a weighted graph with 100 vertices, 1000 edges, and three groups of vertices.
We generated every edge randomly, and if the two vertices were in the same group, we set the edge weight to 1.0; otherwise, we set it to 0.1. It exhibits both strong and weak connections.
\Cref{fig:weightedDense} shows the difference between the two initialization.
When we ignore the edge weights and just solve the problem~\eqref{eq:sa}, the graph is just an Erdos--Renyi graph, and thus \textsf{SA} cannot find any meaningful structure in the initial placement.
On the other hand, the proposed algorithm can find the graph's structure, and we can observe that the left graph in \cref{fig:weightedDense} is separated by the groups, i.e., the node color.
This result suggests that our proposed algorithm is effective even for weighted graphs, extending the applicability of the pre-processing step.

\begin{figure*}[t]
  \centering
  \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[height=2.2cm]{whyRSNfail/whyRSNfail.pdf}
    \caption{
      The inaccurate quadratic approximation.
      For the red vertex on the right graph, its coordinate Newton direction is the red arrow, which is a bad direction.
    }
    \label{fig:whyRSNfail}
  \end{minipage}
  \begin{minipage}{\columnwidth}
    \centering
    \includegraphics[height=2.2cm]{whyRSNfail2/whyRSNfail2.pdf}
    \caption{
      The ignorance of other vertex movements.
      Although the blue arrows show the forces in this situation, the red vertex barely moves by the coordinate Newton direction.
    }
    \label{fig:whyRSNfail2}
  \end{minipage}
\end{figure*}


\section{Challenges of Coordinate Newton Direction}\label{sec:challenges}

This section explains why we took a roundabout approach to solve the problem~\eqref{eq:fr}.
As we have explained, we first transformed it into the discrete optimization problem, the problem~\eqref{eq:frApprox0}, then optimized it using the coordinate Newton direction.
Instead, we can naturally consider directly applying the coordinate Newton direction to optimize the problem~\eqref{eq:fr}. Is it still effective?
We think the answer is no, and in this section, we explain the reasons behind this conclusion and the rationale for our approach.

\subsection{Possible Approach with Coordinate Newton Direction}\label{ssec:possibleApproach}

We first explain a possible approach using the coordinate Newton direction to optimize the problem~\eqref{eq:fr}.
As mentioned, our approach is based on the stochastic coordinate descent and is also resembles the randomized subspace Newton, one of the subspace methods~\cite{NEURIPS2019_bc6dc48b,
  fujiRandomizedSubspaceRegularized2022,
  cartisRandomisedSubspaceMethods2022,
  nozawaRandomizedSubspaceGradient2023,
  higuchiFastConvergenceSecondOrder2024}.

The direct application of such method to the problem~\eqref{eq:fr} is as follows: we randomly select a vertex $i$, apply Newton's method or its regularized variant to $f_i$ using the gradient in \cref{eq:gradientFi} and its Hessian:
\begin{gather*}
  \nabla^2 f_i(x_i) = \sum_{j \neq i} \left(\frac{w_{i,j}\norm{x_i - x_j}}{k} - \frac{k^2}{\norm{x_i - x_j}^2}\right) \mqty(1&0\\0&1)+ \\
  \sum_{j \neq i} \left(\frac{w_{i,j}}{k \norm{x_i - x_j}} + \frac{2k^2}{\norm{x_i - x_j}^4}\right) (x_i - x_j)(x_i - x_j)^\top.
\end{gather*}
Then, we update the position of vertex $i$ and repeat this process until convergence. We do not go through any discrete optimization problem with this approach.
However, this approach fails to work effectively in practice.
In the following, we explain the reasons behind this failure.

\subsection{Inaccuracy of Quadratic Approximation}\label{ssec:inaccuracy}

One of the reasons why it fails is the inaccuracy of quadratic approximation; particularly, a specific issue arises when restricting the optimization to a coordinate block.

We show an example of this issue in \cref{fig:whyRSNfail}.
Let a graph $G$ be the one on the left, where $k$ and all positive edge weights $w_{i,j}$ are 1.
In the situation depicted on the right, the position of points are
\begin{equation*}
  X = \begin{pmatrix}
    0 & -1 & -0.85  & -0.85  & 1 \\
    0 & 0  & +0.155 & -0.155 & 0
  \end{pmatrix}.
\end{equation*}
% todo
The key point of this example is that the Hessian
\begin{equation*}
  \nabla^2 f_2(x_2) \approx \mqty(1.841&0\\0&1.159)
\end{equation*}
is both positive definite and not ill-conditioned, which means that the Newton direction is good in general. Despite such a suitable property of the Hessian, the Newton direction for $x_2$ (red arrow) is a bad direction, leading to a significant deviation from the global optimal solution as it is. This badness comes from the inaccurate approximation of $f$ in the restricted block coordinates $x_2$.
We cannot completely resolve this illness by modifying Newton's method, which we use for $f_i$. This is an inherent and inevitable issue of the coordinate Newton direction.

\subsection{Ignorance of Other Vertex Movements}\label{ssec:ignorance}

Another reason is the ignorance of other vertex movements when optimizing each vertex individually.
When optimizing for a vertex $i$, the coordinate Newton direction treats all other vertices $j (j \neq i)$ as fixed.

Figure~\ref{fig:whyRSNfail2} illustrates this issue.
Consider a subset of vertices that form a mesh-like structure in $G$, and all vertices receive forces to the blue arrow directions.
In this setting, the FR and L-BFGS algorithms progress the optimization without issue.
On the other hand, if we attempt to optimize only for the red vertex $1$ in the right graph, due to its fixed directly connected neighbors, $1$ barely moves.
As a result, the overall optimization barely advances, in contrast to the alignment of the forces (blue arrows).

In this way, ignoring the direction of forces on other vertices can be a significant shortcoming of the coordinate Newton direction.

\subsection{Rationale for Proposed Method}\label{ssec:rationale}

As explained, we suspect that while the coordinate Newton direction effectively reduces the twist in a rough sense, it is unsuitable for optimizing the overall placement.

However, by converting the problem as stated in Section~\ref{ssec:reduction}, we can mitigate these issues.
The advantage of the problem~\eqref{eq:frApprox0} is that not only reduction of the computational complexity from $\order{\abs{V}^2}$ to $\order{\abs{E}}$.
It also brings the convexity of the problem, making the quadratic approximation more accurate than the original problem.
Moreover, the discrete point set $Q$ mitigates the adverse effects caused by ignoring the movement of other vertices. This is because each point is always separated by at least $\epsilon$, making minor movements negligible and non-critical. The stepsize in the optimization is also assured to be more than $\epsilon$, evading the stagnate of the optimization.
These advantages are the key to preventing the issues of \cref{ssec:ignorance} and \cref{ssec:inaccuracy}.
Thus, making the problem discrete is essential to provide a high-quality initial placement.

The crucial point is that, when combined with suitable strategies, the stochastic coordinate descent or randomized subspace Newton can effectively solve the original continuous problem, the problem~\eqref{eq:fr}.
Focusing on each vertex separately is a natural and valid approach.
With further refinements and by addressing the issues discussed in \cref{ssec:inaccuracy} and \cref{ssec:ignorance}, these methods hold the potential to efficiently provide good solutions for the problem~\eqref{eq:fr}.

\section{Discussion} \label{sec:discussion}

In this section, we discuss the future directions of this research.
Firstly, we discuss combining our algorithm with conventional techniques, such as the multilevel approach.
Secondly, we explore the applications beyond the scope of graph drawing.
Finally, we conclude this paper.

\subsection{Combination with Other Techniques}\label{sec:combination}

This paper has demonstrated the effectiveness of the proposed method on various graphs, but we can also apply it to larger-scale problems.
In general, approximating or simplifying the model itself is one of the strategies to deal with large graphs.
The $n$-body simulation using multipole expansions~\cite{greengardFastAlgorithmParticle1987}, approximating by the Barnes--Hut approximation~\cite{barnesHierarchicalLogForcecalculation1986}, gradually refining the layouts using a multilevel approach~\cite{Hu2006EfficientHF}, and employing stress majorization~\cite{gansnerGraphDrawingStress2005} are examples of such approaches.
For instance, the Scalable Force-Directed Placement (sfdp) of Graphviz~\cite{ellsonGraphvizOpenSource2002}, based on \cite{Hu2006EfficientHF}, employs a multilevel approach to accelerate processing for larger graphs by progressively coarsening vertices.

In particular, the coarsening operation in sfdp does not critically conflict with the proposed method, making it feasible to combine both approaches.
Specifically, by iteratively applying the proposed method to the entire coarsened graph or groups of vertices consolidated through coarsening, it is possible to extend its applicability to larger-scale problems.
We can expect this approach to yield faster and higher-quality solutions.
Addressing this integration is one of the challenges for future research.

In addition, the FR force model is sometimes used not only in $\bbR^2$ but also in $\bbR^3$~\cite{14738716211060306}. Although we have to modify some parts of the proposed algorithm for $\bbR^3$, such as the hexagonal lattice, its application would be easy.

\subsection{Application to Other Problems}\label{ssec:application}

In this subsection, we briefly discuss and explore the potential applicability of stochastic coordinate descent to a broader range of problems.
Although we utilized the coordinate Newton direction only for the optimization problem~\eqref{eq:fr}, we can see that its application is not necessarily limited to the FR force model alone.

In general, the optimization problem~\eqref{eq:fr} is more broadly treated as ``objective functions arising from graphs''~\cite{recht-wright}:
\begin{mini*}
  {X \in \bbR^{2 \times n}}
  {f(X) = \sum_{\{i,j\}\in E} f_{i,j}(x_i, x_j)+\lambda \sum_{i=1}^{n} \Omega_i(x_i),}
  {}
  {}
\end{mini*}
where $\Omega_i$ is a regularization term for vertex $i$ and $\lambda$ is a regularization parameter.
The optimization problem~\eqref{eq:fr} is a special case of this problem class.
The authors of~\cite{recht-wright} claim that coordinate descent, based only on coordinate gradients, effectively solves such problems. A variant of the proposed method utilizing the coordinate Newton direction can also be effective for such problems.

For instance, we suspect that the graph isomorphism problem is one of the candidates for the application.
The graph isomorphism problem is a well-known combinatorial optimization problem to determine whether two graphs $G_1, G_2$ are isomorphic, i.e., $G_1 \cong G_2$.
The graph isomorphism problem is closely related to the graph drawing. Drawing a graph in a way that reveals its symmetry is at least as difficult as the graph isomorphism problem~\cite{eades1984heuristic}. Indeed, if we can draw two graphs $G_1$ and $G_2$ in the same way, it becomes evident that $G_1 \cong G_2$. See \cref{fig:iso} for reference.
When we relax it to a continuous optimization problem on Riemannian manifolds~\cite{klusContinuousOptimizationMethods2023,klusContinuousOptimizationMethods2023}, we might be able to apply the stochastic coordinate descent or coordinate Newton direction to this problem as well.
Investigating the variant of our proposed algorithm for these problems constitutes one of the future research directions.

\begin{figure}[t]
  \centering
  \includegraphics[width=\columnwidth]{iso/iso.pdf}
  \caption{
    Illustration of the relationship between graph drawing and graph isomorphism.
    If we can draw graphs $G_1$ and $G_2$ symmetrically, then it is clear that $G_1 \cong G_2$.
  }
  \label{fig:iso}
\end{figure}

\subsection{Conclusion} \label{sec:conclusion}

In this study, we proposed a new initial placement with the coordinate Newton direction for the FR force model.
The obtained initial placements have fewer twists than the random initialization, leading to faster convergence and better outcomes.
To demonstrate its effectiveness, we conducted numerical experiments, which revealed that the proposed method is effective across various graphs, extending the applicability of the pre-processing step.

We expect that the proposed method may advance the graph drawing of the FR force model and highlight the potential of the stochastic coordinate descent and its variants for addressing a broader range of graph-related optimization problems.

\section{Acknowledgment}

The author would like to express our sincere gratitude to PL Poirion and Andi Han for their insightful discussions, which have greatly inspired and influenced this research.

The author also thanks the developers of NetworkX and Graphviz. Their excellent work has been a great help in conducting this research.

This work was partially supported by JSPS KAKENHI (23H03351,24K23853) and JST ERATO (JPMJER1903).

\ifthenelse{\boolean{isMain}}{
  \bibliographystyle{IEEEtranSN}
  \bibliography{bstcontrol,FruchtermanReingoldByRandomSubspace}
}{}

\appendices

\section{Algorithm to solve the problem~\ref{eq:fr}}\label{sec:algToProb1}

This section explains how to solve the problem~\eqref{eq:fr} and obtain the final placement for the graph drawing.

\subsection{Fruchterman--Reingold Algorithm}\label{ssec:frAlgorithm}

The Fruchterman--Reingold algorithm~\cite{fruchtermanGraphDrawingForcedirected1991} is the original force-directed algorithm and the most standard approach for the FR force model.

Let $f_i\colon \bbR^2 \to \bbR$ denote the energy function for the vertex $i$ at $x_i$. This is defined by
\begin{equation}\label{eq:fi}
  f_i(x_i) \defeq \sum_{j \neq i} E_{i,j}(\norm{x_i - x_j}),
\end{equation}
and its gradient, the sum of forces acting on the vertex $i$, is
\begin{equation}\label{eq:gradientFi}
  \nabla f_i(x_i) = \sum_{j \neq i} \qty(\frac{w_{i,j}\norm{x_i - x_j}}{k} - \frac{k^2}{\norm{x_i - x_j}^2}) (x_i-x_j).
\end{equation}

The pseudo-code of the FR algorithm is shown in \cref{alg:fr}, which can be regarded as a variant of gradient (steepest) descent method for the function $f$~\cite{tunkelang1999numerical}.
\begin{algorithm}[ht]
  \caption{Fruchterman--Reingold algorithm}
  \label{alg:fr}
  \KwIn{Graph $G = (V, E)$, Weights $(w_{i,j})_{\{i,j\} \in E}$, Parameters $N_\mathrm{iter}^\mathsf{FR} \in \bbN$, $t_0 > 0$, Initial placement $X$}
  \KwOut{Final placement $X = (x_1, \dots, x_n)$}

  $t \gets t_0$\;
  \For{$m \gets 1$ \KwTo $N_\mathrm{iter}^\mathsf{FR}$}{
    $\text{compute gradient } \nabla f_i(x_i)$ for all $i \in V$\;
    $x_i^\mathrm{new} \gets x_i - t \frac{\nabla f_i(x_i)}{
        \norm{\nabla f_i(x_i)}}$ for all $i \in V$\;
    $x_i \gets x_i^\mathrm{new}$\;
    $t \gets t - t_0 / N_\mathrm{iter}^\mathsf{FR}$\;
    \If{\text{convergence condition is satisfied}}{
      \textbf{break}\;
    }
  }
  \Return $X$\;
\end{algorithm}

The \cref{alg:fr} is based on the original code~\cite{fruchtermanGraphDrawingForcedirected1991} and implementation in NetworkX~\cite{hagberg2008exploring} with some omitted details.
The initial placement $X$ is often drawn from a uniform distribution on a unit square.
The parameter $t$ denotes the temperature, which governs the stepsize along the steepest descent. As the temperature gradually decreases, the algorithm converges to a particular placement, though this placement is not necessarily the local optimum to the problem~\eqref{eq:fr}.

\subsection{L-BFGS Algorithm}\label{ssec:lbfgs}

\begin{figure}[t]
  \centering
  \includegraphics[height=3.59cm]{comparison/comparison_FRandLBFGS.pdf}
  \caption{
    Comparison of the FR algorithm and the L-BFGS algorithm.
    While the FR algorithm moves vertices in a descent direction with a fixed stepsize (blue arrows), the L-BFGS algorithm adjusts them differently since it utilizes approximated inverse Hessian (orange arrows).
  }
  \label{fig:comparisonFRandLBFGS}
\end{figure}

Another approach to solving the optimization problem~\eqref{eq:fr} is to use the Limited-memory Broyden--Fletcher--Goldfarb--Shanno (L-BFGS) algorithm~\cite{6183577}.
Using only a few recent gradient vectors, the L-BFGS algorithm approximates the inverse Hessian of the objective function $f$~\cite{liuLimitedMemoryBFGS1989}.
L-BFGS is known to be very efficient for large-scale optimization problems, and the superior performance of the L-BFGS algorithm to the FR algorithm reported in Ref.~\cite{6183577} also indicates this fact. Refer to \cref{fig:comparisonFRandLBFGS} for a comparison to the FR algorithm.

For the optimization problem~\eqref{eq:fr}, we can apply the L-BFGS algorithm via flattening the matrix $X \in \bbR^{2 \times n}$ to a vector $\overline{X} \in \bbR^{2n}$.
It is worth noting that this method ignores the structure of $X$ and treats it just as a general optimization problem.

% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Hiroki Hamaguchi}
%   Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
% \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Naoki Marumo}
%   Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
% \end{IEEEbiography}
% \begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{fig1}}]{Akiko Takeda}
%   Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
%   Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan.
% \end{IEEEbiography}

\begin{IEEEbiographynophoto}{Hiroki Hamaguchi}
  Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
\end{IEEEbiographynophoto}
\begin{IEEEbiographynophoto}{Naoki Marumo}
  Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
\end{IEEEbiographynophoto}
\begin{IEEEbiographynophoto}{Akiko Takeda}
  Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan.
  Center for Advanced Intelligence Project, RIKEN, Tokyo, Japan.
\end{IEEEbiographynophoto}

\end{document}